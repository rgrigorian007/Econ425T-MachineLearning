{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Econ 425T Homework 2\n",
    "author: \"Richard Grigorian (UID: 505-088-797)\"\n",
    "date: today\n",
    "format: \n",
    "    html:\n",
    "        mainfont: \"Palantino\"\n",
    "        fontsize: medium\n",
    "        number-sections: true\n",
    "        number-depth: 3\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        code-fold: false\n",
    "        html-math-method: katex\n",
    "        embed-resources: false\n",
    "        self-contained-math: false\n",
    "    pdf:\n",
    "        mainfont: Times New Roman\n",
    "        sansfont: Times New Roman\n",
    "        number-sections: True\n",
    "        number-depth: 2\n",
    "        toc: true\n",
    "        toc-depth: 2\n",
    "        toc-title: Contents\n",
    "        shift-heading-level-by: -1\n",
    "        execute:\n",
    "            warning: false\n",
    "            cache: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< pagebreak >}}\n",
    "\n",
    "## Least squares is MLE\n",
    "\n",
    "Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "To prove the above statement, we separate the proof into two pieces. First, we will show that maximum likelihood and least squares result in the same $\\beta$ coefficient under linear models with Gaussian errors. Then we will show the equivlanece of $C_p$ and AIC.\n",
    "\n",
    "**Part I:**\n",
    "\n",
    "We want to show that\n",
    "$$ \\hat{\\beta}_{ML} = \\underset{\\beta \\in \\mathbb{R}^{p+1}}{\\operatorname{arg \\ max}} \\enspace \\mathcal{L}(\\beta) = (X'X)^{-1}X'Y = \\hat{\\beta}_{OLS}$$\n",
    "Under linear models, we assume Linearity such that\n",
    "$$\\mathbb{E}[Y | X_1 = x_1 , \\dots, X_p = x_p] = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p$$\n",
    "Normal (Gaussian) errors imply that $\\varepsilon \\sim N(0,\\sigma^2)$. Hence, for a sample $\\{(x_i, y_i) \\}^n_{i=1}$, we say $Y|X \\sim N(X'\\beta, \\sigma^2 I)$ where $I_{n\\times n}$ is the Identity matrix. Since $Y_1 , \\dots, Y_n$ is i.i.d., we obtain the likelihood function\n",
    "$$ \\mathcal{L} = \\prod^n_{i=1} \\phi (Y_i ; (X'\\beta)_i, \\sigma^2 I) = \\prod^n_{i=1} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-(Y_i -X'\\beta)^2}{2 \\sigma^2}} $$\n",
    "We can re-write this the likelihood function in matrix form as\n",
    "$$ \\mathcal{L} = \\left ( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(\\frac{-(Y-X\\beta)'(Y-X\\beta)}{2 \\sigma^2}\\right)$$\n",
    "Where $Y_{n \\times 1}$ and $X_{n \\times p}$. We can then take the logarithm of both sides to get the log-likelihood function:\n",
    "\\begin{align*}\n",
    "\\ln (\\mathcal{L}) &= - \\frac{n}{2} \\ln(2 \\pi) - \\frac {n}{2} \\ln(\\sigma^2) - \\frac{1}{2 \\sigma^2} (Y-X\\beta)'(Y-X\\beta) \\\\\n",
    "&= - \\frac{n}{2} \\ln(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2 \\sigma^2} (Y'Y - 2\\beta X'Y + \\beta'X'X\\beta)\n",
    "\\end{align*}\n",
    "Since the transpose of a scalar is the scalar $Y'X\\beta = (Y'X\\beta)' = \\beta'X'Y$. We now take the partial derivative with respect to $\\beta$:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln (\\mathcal{L})}{\\partial \\beta} &= - \\frac{1}{2 \\sigma^2} \\left ( \\frac{\\partial [Y'Y - 2\\beta X'Y + \\beta'X'X\\beta]}{\\partial \\beta} \\right) = 0 \\\\\n",
    "&= - \\frac{1}{2 \\sigma^2} (-2X'Y + 2X'X\\beta) = \\frac{1}{\\sigma^2} (X'Y - X'X\\beta) = 0\n",
    "\\end{align*}\n",
    "This then implies that\n",
    "$$\\hat{\\beta}_{ML} = (X'X)^{-1}X'Y = \\hat{\\beta}_{OLS}$$\n",
    "\n",
    "**Part II:**\n",
    "\n",
    "We now want to show the equivalence between $C_p$ and AIC under the same assumptions as in Part (I). We know from lecture that\n",
    "$$C_p = \\frac{1}{n} (\\text{RSS} + 2d \\hat{\\sigma}^2)$$\n",
    "And\n",
    "$$\\text{AIC} = -  2 \\ln \\mathcal{L} + 2d$$\n",
    "where $d$ is the total number of parameters used and $\\hat{\\sigma}^2$ is an estimate of the error variance $\\text{Var}(\\varepsilon)$. Hence, we substitute in our expression for $\\mathcal{L}$ from Part (I):\n",
    "\\begin{align*}\n",
    "\\text{AIC} &= -  2 \\ln \\mathcal{L} + 2d \\\\\n",
    "&= -2 \\left( - \\frac{n}{2} \\ln(2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{(Y - \\hat{Y})'(Y - \\hat{Y})}{2 \\sigma^2} \\right) + 2d \\\\\n",
    "&= n \\ln (2\\pi \\sigma^2) + \\frac{(Y - \\hat{Y})'(Y - \\hat{Y})}{\\sigma^2} + 2d \\\\\n",
    "&= n \\ln (2\\pi \\sigma^2) + \\frac{\\text{RSS}}{\\sigma^2} + 2d \\\\\n",
    "&= n \\ln (2\\pi \\sigma^2) + \\frac{n}{\\sigma^2} C_p\n",
    "\\end{align*}\n",
    "Notice, that AIC only differs by $C_p$ by constant scalars $n$ and $\\sigma$. Therefore, both metrics will coincide in terms of model selection. Hence, we show that the $C_p$ and AIC are equivalent in this sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 6.6.1 (10pts)\n",
    "\n",
    "We perform best subset, forward stepwise, and backward stepwise\n",
    "selection on a single data set. For each approach, we obtain $p + 1$\n",
    "models, containing $0,1,2,\\dots,p$ predictors. Explain your answers:\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "Which of the three models with $k$ predictors has the smallest\n",
    "training RSS?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Best subset selection will be the model with the smallest training RSS. This is due to the fact that the other 2 approaches conduct model selection dependent upon their starting predictor. In the sense that, if forward/backward selection chooses a different starting variable, the final model may be different. Hence, the RSS will be smallest for Best subset which runs through ever possible combination of model features.\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "Which of the three models with $k$ predictors has the smallest\n",
    "test RSS?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Best subset selection will likely be the model also with the smallest testing RSS. Once again, it is due to the construction of best subset where it will consider a much greater selection of models. In fact, it considers all combinations of model features. Hence, it is likely that it will have a smaller test RSS than either forward/backward stepwise selection.\n",
    "\n",
    "### Part (c) True or False:\n",
    "\n",
    "1. The predictors in the $k$-variable model identified by forward\n",
    "stepwise are a subset of the predictors in the $(k+1)$ variable\n",
    "model identified by forward stepwise selection.\n",
    "\n",
    "This is **True**. This is literally how forward stepwise selection works. It has a base model (with 1 feature) and then it iteratively adds features and drops those that become statistically insignificant. hence, the $k$ feature model will be a subset of the $k+1$ feature model.\n",
    "\n",
    "2. The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the $(k + 1)$\n",
    "variable model identified by backward stepwise selection.\n",
    "\n",
    "This is **True**. Once again the model with $k$ features is just the model with $k+1$ features except we have removed a variable. Hence, it is a subset.\n",
    "\n",
    "3.  The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the $(k + 1)$\n",
    "variable model identified by forward stepwise selection.\n",
    "\n",
    "This is **False**. Forward and Backward selection are not linked in this kind of way. They are also somewhat opposite processes, so the features in the $k$ variable model of one method is not guaranteed to be a subset of the $k+1$ feature model of the other method.\n",
    "\n",
    "4. The predictors in the $k$-variable model identified by forward\n",
    "stepwise are a subset of the predictors in the $(k+1)$ variable\n",
    "model identified by backward stepwise selection.\n",
    "\n",
    "This is **False** for the same reason. Forward and Backward selection are not linked in this kind of way. They are also somewhat opposite processes, so the features in the $k$ variable model of one method is not guaranteed to be a subset of the $k+1$ feature model of the other method.\n",
    "\n",
    "5.  The predictors in the $k$-variable model identified by best\n",
    "subset are a subset of the predictors in the $(k + 1)$ variable\n",
    "model identified by best subset selection.\n",
    "\n",
    "This is **False**. This is not necessarily true because best subset tests every combination of features from the available set of features. By increasing the set of features by 1, it is possible that best subset selection will arrive at a combination of features that were not all simultaneously chosen by the $k$ case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 6.6.3 (10pts)\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression\n",
    "model by minimizing\n",
    "\n",
    "$$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr)\\text{ subject to }\\sum_{j=1}^p|\\beta_j|\\le s$$\n",
    "\n",
    "for a particular value of $s$. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "As we increase $s$ from 0. the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a\n",
    "U shape\n",
    "\n",
    "iii. Steadily increase\n",
    "\n",
    "iv. Steadily decrease\n",
    "\n",
    "v. Remain constant.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iv.** is correct. The training RSS will steadily decrease as we increase $s$ from 0. As we increas $s$ we are weakening our restriction on what our $\\beta$ coefficients can be; therefore, the model is becoming more flexible which causes a steady decrease in training RSS.\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "Repeat (a) for test RSS.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**ii.** is correct. We call on the same idea as before. As we weaken the restriction, the coefficients are allowed to increase to their least squares estimates. This is what causes the testing RSS to initially decrease before increasing once again when the coefficients start overfitting and the variance increases.\n",
    "\n",
    "### Part (c)\n",
    "\n",
    "Repeat (a) for variance.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iii.** is correct. As hinted to in the previous part, when $s=0$ the coefficients will have no variance. As the restriction is loosened, the model coefficients begin to increase and the coefficients become increasingly fitted to the training data. Hence, the variance will increase.\n",
    "\n",
    "### Part (d)\n",
    "\n",
    "Repeat (a) for (squared) bias.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iv.** is correct. We call on the classic bias-variance trade-off. As we loosen the restriction, we are imposing less and less shrinkage; hence, the bias decreases monotonically.\n",
    "\n",
    "### Part (e)\n",
    "\n",
    "Repeat (a) for the irreducible error.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**v.** is correct. By definition, the irreducible error is due to the underlying data rather than any choice of model or restriction. Hence, it will always stay constant regardless of whatever restrictions we impose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 6.6.4 (10pts)\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) - \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "for a particular value of $\\lambda$. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "As we increase $\\lambda$ from 0. the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a\n",
    "U shape\n",
    "\n",
    "iii. Steadily increase\n",
    "\n",
    "iv. Steadily decrease\n",
    "\n",
    "v. Remain constant.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iii.** is correct. The training RSS will steadily increase as we increase $\\lambda$ from 0. As we increas $\\lambda$ we are strenghtening our restriction on what our $\\beta$ coefficients can be; therefore, the model is becoming less flexible which causes a steady increase in training RSS.\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "Repeat (a) for test RSS.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**ii.** is correct. We call on the same idea as before. As we strenghten the restriction, the coefficients are allowed to deviate further and further from their Least Squares estimates. Hence, as we increase $\\lambda$, the coefficients start being reduced further and further to zero and the overfitting is reduced. Thus, test RSS initially decreases but then when all coefficients are approaching 0, the model is clearly not correct and we expect test RSS to increase.\n",
    "\n",
    "### Part (c)\n",
    "\n",
    "Repeat (a) for variance.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iv.** is correct. As hinted to in the previous part, as $\\lambda \\to \\infty$ the coefficients will all be shrunken to 0 and hence have no variance. Thus, as $\\lambda$ increases, we expect variance to monotonically decrease.\n",
    "\n",
    "### Part (d)\n",
    "\n",
    "Repeat (a) for (squared) bias.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**iii.** is correct. We call on the classic bias-variance trade-off. As we strengthen the restriction, we are imposing more and more shrinkage; hence, the bias increases monotonically.\n",
    "\n",
    "### Part (e)\n",
    "\n",
    "Repeat (a) for the irreducible error.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "**v.** is correct. By definition, the irreducible error is due to the underlying data rather than any choice of model or restriction. Hence, it will always stay constant regardless of whatever restrictions we impose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 6.6.5 (10pts)\n",
    "\n",
    "It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give different coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "Suppose that $n=2$, $p=2$, $x_{11} = x_{12}, x_{21} = x_{12}$. Furthermore, suppose that $y_1 + y_2 = 0$ and $x_{11} + x_{21} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{\\beta}_0 = 0$.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "Write out the ridge regression optimization problem in this setting.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Recall that Ridge Regression seeks to find the coefficients that minimize\n",
    "$$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) - \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "Hence, with the given information, we want to minimize:\n",
    "$$(y_1 - \\hat{\\beta}_1x_1 - \\hat{\\beta}_2x_1)^2 + (y_2 - \\hat{\\beta}_1x_2 - \\hat{\\beta}_2x_2)^2 + \\lambda(\\hat{\\beta}_1^2 + \\hat{\\beta}_2^2)$$\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "Argue that in this setting, the ridge coefficient estimates satisfy $\\hat{\\beta}_1 = \\hat{\\beta}_2$.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "We accomplish this by taking the partial derivative with respect to both $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$. Hence, we have\n",
    "$$\\frac{\\partial(\\cdot)}{\\partial \\hat{\\beta}_1} = -2x_1 (y - \\hat{\\beta}_1 x_1 - \\hat{\\beta}_2 x_1) - 2x_2 (y_2 - \\hat{\\beta}_1 x_2 - \\hat{\\beta}_2 x_2) + 2 \\lambda \\hat{\\beta}_1 = 0$$\n",
    "Which then implies that\n",
    "$$\\hat{\\beta}_1 = \\frac{x_1 (y - \\hat{\\beta}_1 x_1 - \\hat{\\beta}_2 x_1) + x_2 (y_2 - \\hat{\\beta}_1 x_2 - \\hat{\\beta}_2 x_2)}{\\lambda}$$\n",
    "If we then take the partial with respect to $\\hat{\\beta}_2$ we get\n",
    "$$\\frac{\\partial(\\cdot)}{\\partial \\hat{\\beta}_2} = -2x_1 (y - \\hat{\\beta}_1 x_1 - \\hat{\\beta}_2 x_1) - 2x_2 (y_2 - \\hat{\\beta}_1 x_2 - \\hat{\\beta}_2 x_2) + 2 \\lambda \\hat{\\beta}_2 = 0$$\n",
    "Which then can be re-written as\n",
    "$$\\hat{\\beta}_2 = \\frac{x_1 (y - \\hat{\\beta}_1 x_1 - \\hat{\\beta}_2 x_1) + x_2 (y_2 - \\hat{\\beta}_1 x_2 - \\hat{\\beta}_2 x_2)}{\\lambda}$$\n",
    "This can only be true if $\\hat{\\beta}_1 = \\hat{\\beta}_2$,\n",
    "\n",
    "### Part (c)\n",
    "\n",
    "Write out the lasso optimization problem in this setting.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Recall that Lasso seeks to find the coefficients that minimize\n",
    "$$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) - \\lambda\\sum_{j=1}^p |\\beta_j |$$\n",
    "Hence, with the given information, we want to minimize:\n",
    "$$(y_1 - \\hat{\\beta}_1x_1 - \\hat{\\beta}_2x_1)^2 + (y_2 - \\hat{\\beta}_1x_2 - \\hat{\\beta}_2x_2)^2 + \\lambda(|\\hat{\\beta}_1| + |\\hat{\\beta}_2|)$$\n",
    "\n",
    "### Part (d)\n",
    "\n",
    "Aruge that in this setting, the Lasso coefficients $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ are not unique -- in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Our approach to arguing the lack of uniquness begins with us re-writing our Lasso minimization problem in the form in which it was presented in problem 3 (ISL 6.6.3)\n",
    "$$(y_1 - \\hat{\\beta}_1x_1 - \\hat{\\beta}_2x_1)^2 + (y_2 - \\hat{\\beta}_1x_2 - \\hat{\\beta}_2x_2)^2$$\n",
    "Subject to the constraint\n",
    "$$ |\\hat{\\beta}_1| + |\\hat{\\beta}_2|\\le s $$\n",
    "Now, we know (from lecture) that the constraint in Lasso geometrically is a diamond at the origin of $\\mathbb{R}^2$ euclidean plane ($\\hat{\\beta}_1 , \\hat{\\beta}_2$) with a distance from the axis equal to the constraint $s$. Beyond this, we can further simplify our expression since our problem parameters allow us to collapse several terms in order to get\n",
    "$$2(y_1 - (\\hat{\\beta}_1 + x_1\\hat{\\beta}_2))^2\\ge 0$$\n",
    "Now we minimize this problem by taking the partial\n",
    "$$\\frac{\\partial(\\cdot)}{\\partial (\\hat{\\beta}_1 + \\hat{\\beta}_2)} = -4x_1(y_1 - x_1 (\\hat{\\beta}_1 + \\hat{\\beta}_2)) = 0$$\n",
    "Which then simply rearranges to show\n",
    "$$\\hat{\\beta}_1 + \\hat{\\beta}_2 = \\frac{y_1}{x_1}$$\n",
    "This expression represents a line which is parallel to the edge of the Lasso diamond. Therefore, the entire edge of the diamond that intersects with the lasso function serves as our solution set. This shows that the solution to this particular Lasso problem is not unique. In particular the lasso optimization problem as the set of solutions\n",
    "$$\\{(\\hat{\\beta}_1,\\hat{\\beta}_2) : \\hat{\\beta}_1 + \\hat{\\beta}_2 = s, \\ \\hat{\\beta}_1 , \\hat{\\beta}_2\\ge 0\\}$$\n",
    "And the solutions are symmetric for the case where it is less than zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 6.6.11 (30pts)\n",
    "\n",
    "You must follow the [typical machine learning paradigm](https://ucla-econ-425t.github.io/2023winter/slides/06-modelselection/workflow_lasso.html) to compare *at least* 3 methods: least squares, lasso, and ridge. Report final results as\n",
    "\n",
    "| Method | CV RMSE | Test RMSE |\n",
    "|:------:|:------:|:------:|:------:|\n",
    "| LS | | | |\n",
    "| Ridge | | | |\n",
    "| Lasso | | | |\n",
    "| ... | | | |\n",
    "\n",
    "We will now try to predict the per capita crime rate in the `Boston` data set.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "Try out some of the regression methods explored in this chapter,\n",
    "such as best subset selection, the lasso, ridge regression, and\n",
    "PCR. Present and discuss results for the approaches that you\n",
    "consider.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "As specified above, we will use the *typical machine learning paradigm* to approach this problem. Namely, we will consider least squares, lasso, and ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "   lstat  medv  \n",
       "1   4.98  24.0  \n",
       "2   9.14  21.6  \n",
       "3   4.03  34.7  \n",
       "4   2.94  33.4  \n",
       "5   5.33  36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Loading Data\n",
    "url = \"https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/Boston.csv\"\n",
    "s = requests.get(url).content\n",
    "Boston = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col = 0)\n",
    "Boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>506.0</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.25651</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>88.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>506.0</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>506.0</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.46000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>9.69000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>27.7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>506.0</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>506.0</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.38500</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.53800</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.8710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>506.0</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>3.56100</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>6.20850</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>8.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>506.0</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.90000</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>77.50000</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>506.0</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>1.12960</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>3.20745</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>12.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>506.0</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>506.0</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>187.00000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>330.00000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>711.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>506.0</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>12.60000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>19.05000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>22.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>506.0</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>1.73000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>11.36000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>37.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>506.0</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>9.197104</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>17.025000</td>\n",
       "      <td>21.20000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>50.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count        mean         std        min         25%        50%  \\\n",
       "crim     506.0    3.613524    8.601545    0.00632    0.082045    0.25651   \n",
       "zn       506.0   11.363636   23.322453    0.00000    0.000000    0.00000   \n",
       "indus    506.0   11.136779    6.860353    0.46000    5.190000    9.69000   \n",
       "chas     506.0    0.069170    0.253994    0.00000    0.000000    0.00000   \n",
       "nox      506.0    0.554695    0.115878    0.38500    0.449000    0.53800   \n",
       "rm       506.0    6.284634    0.702617    3.56100    5.885500    6.20850   \n",
       "age      506.0   68.574901   28.148861    2.90000   45.025000   77.50000   \n",
       "dis      506.0    3.795043    2.105710    1.12960    2.100175    3.20745   \n",
       "rad      506.0    9.549407    8.707259    1.00000    4.000000    5.00000   \n",
       "tax      506.0  408.237154  168.537116  187.00000  279.000000  330.00000   \n",
       "ptratio  506.0   18.455534    2.164946   12.60000   17.400000   19.05000   \n",
       "lstat    506.0   12.653063    7.141062    1.73000    6.950000   11.36000   \n",
       "medv     506.0   22.532806    9.197104    5.00000   17.025000   21.20000   \n",
       "\n",
       "                75%       max  \n",
       "crim       3.677083   88.9762  \n",
       "zn        12.500000  100.0000  \n",
       "indus     18.100000   27.7400  \n",
       "chas       0.000000    1.0000  \n",
       "nox        0.624000    0.8710  \n",
       "rm         6.623500    8.7800  \n",
       "age       94.075000  100.0000  \n",
       "dis        5.188425   12.1265  \n",
       "rad       24.000000   24.0000  \n",
       "tax      666.000000  711.0000  \n",
       "ptratio   20.200000   22.0000  \n",
       "lstat     16.955000   37.9700  \n",
       "medv      25.000000   50.0000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "Boston.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have worked with the Boston Housing Dataset before (in HW #1), we will omit grpahical summaries. Below we will split our data into the major test and non-test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Split into test and non-test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Boston_other, Boston_test = train_test_split(\n",
    "    Boston,\n",
    "    train_size= 0.75,\n",
    "    random_state = 425, # seed\n",
    ")\n",
    "Boston_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston_other.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate non-test X and y\n",
    "X_other = Boston_other.drop('medv', axis = 1)\n",
    "y_other = Boston_other.medv\n",
    "# Test X and y\n",
    "X_test = Boston_test.drop('medv', axis = 1)\n",
    "y_test = Boston_test.medv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move onto the preprocessing step. We do not have categorical variables in our dataset (the only one being `chas` but it is already binary), hence we do not need to use a OneHotEncoder transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Standardization transformer\n",
    "scalar = StandardScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build our model. Namely, we will focus on the 3 mentioned earlier (Least Squares, Lasso, and Ridge).\n",
    "\n",
    "#### Lasso and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(max_iter = 10000)\n",
    "ridge = Ridge(max_iter= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipe_lasso = Pipeline(steps = [\n",
    "    (\"std_tf\", scalar),\n",
    "    (\"model\", lasso)\n",
    "])\n",
    "pipe_ridge = Pipeline(steps = [\n",
    "    (\"std_tf\", scalar),\n",
    "    (\"model\", ridge)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up our tuning grid in the range of $10^{-2}$ to $10^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(start = -3, stop = 2, base = 10, num = 100)\n",
    "tuned_parameters = {\"model__alpha\": alphas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Lasso\n",
    "# Set up CV\n",
    "n_folds = 10\n",
    "search_lasso = GridSearchCV(\n",
    "  pipe_lasso, \n",
    "  tuned_parameters, \n",
    "  cv = n_folds, \n",
    "  scoring = \"neg_root_mean_squared_error\",\n",
    "  # Refit the best model on the whole data set\n",
    "  refit = True \n",
    "  )\n",
    "# Ridge\n",
    "n_folds = 10\n",
    "search_ridge = GridSearchCV(\n",
    "  pipe_ridge, \n",
    "  tuned_parameters, \n",
    "  cv = n_folds, \n",
    "  scoring = \"neg_root_mean_squared_error\",\n",
    "  refit = True \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('std_tf', StandardScaler()),\n",
       "                                       ('model', Ridge(max_iter=10000))]),\n",
       "             param_grid={'model__alpha': array([1.00000000e-03, 1.12332403e-03, 1.26185688e-03, 1.41747416e-03,\n",
       "       1.59228279e-03, 1.78864953e-03, 2.00923300e-03, 2.25701972e-03,\n",
       "       2.53536449e-03, 2.84803587e-03, 3.19926714e-03, 3.59381366e-03,\n",
       "       4.03701726e-03, 4.53487851e-03...\n",
       "       6.89261210e+00, 7.74263683e+00, 8.69749003e+00, 9.77009957e+00,\n",
       "       1.09749877e+01, 1.23284674e+01, 1.38488637e+01, 1.55567614e+01,\n",
       "       1.74752840e+01, 1.96304065e+01, 2.20513074e+01, 2.47707636e+01,\n",
       "       2.78255940e+01, 3.12571585e+01, 3.51119173e+01, 3.94420606e+01,\n",
       "       4.43062146e+01, 4.97702356e+01, 5.59081018e+01, 6.28029144e+01,\n",
       "       7.05480231e+01, 7.92482898e+01, 8.90215085e+01, 1.00000000e+02])},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit CV\n",
    "search_lasso.fit(X_other, y_other)\n",
    "search_ridge.fit(X_other, y_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CV results\n",
    "cv_res = pd.DataFrame({\n",
    "  \"alpha\": alphas,\n",
    "  \"rmse_lasso\": -search_lasso.cv_results_[\"mean_test_score\"],\n",
    "  \"rmse_ridge\": -search_ridge.cv_results_[\"mean_test_score\"]\n",
    "  })\n",
    "\n",
    "sns.relplot(\n",
    "  data = cv_res,\n",
    "  x = \"alpha\",\n",
    "  y = \"rmse_lasso\",\n",
    "  ).set(\n",
    "    xlabel = \"alpha\",\n",
    "    ylabel = \"CV RMSE\",\n",
    "    title = \"Lasso\",\n",
    "    xscale = \"log\"\n",
    ")\n",
    "sns.relplot(\n",
    "  data = cv_res,\n",
    "  x = \"alpha\",\n",
    "  y = \"rmse_ridge\",\n",
    "  ).set(\n",
    "    xlabel = \"alpha\",\n",
    "    ylabel = \"CV RMSE\",\n",
    "    title = \"Ridge\",\n",
    "    xscale = \"log\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, we can clearly see how the CV RMSE changes based upon the selected constraint parameter (in this case alpha, but mathematically $\\lambda$). For Lasso, the minimum seems to be at low alpha values (although a clear minimum is not present). Whereas for Ridge,we see a clear dip around an alpha of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE (Lasso):  4.725006671650144\n",
      "Best CV RMSE (Ridge):  4.721794901043198\n"
     ]
    }
   ],
   "source": [
    "print('Best CV RMSE (Lasso): ',  -search_lasso.best_score_)\n",
    "print('Best CV RMSE (Ridge): ',  -search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model:  Pipeline(steps=[('std_tf', StandardScaler()),\n",
      "                ('model', Lasso(alpha=0.016297508346206444, max_iter=10000))])\n",
      "Ridge model:  Pipeline(steps=[('std_tf', StandardScaler()),\n",
      "                ('model', Ridge(alpha=5.462277217684343, max_iter=10000))])\n",
      "Test RMSE (Lasso):  4.8822192249764385\n",
      "Test RMSE (Ridge):  4.880408916408217\n"
     ]
    }
   ],
   "source": [
    "# Finalize model\n",
    "print(\"Lasso model: \", search_lasso.best_estimator_)\n",
    "print(\"Ridge model: \", search_ridge.best_estimator_)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lasso_rmse_final = mean_squared_error(y_test,\n",
    " search_lasso.best_estimator_.predict(X_test), squared = False)\n",
    "ridge_rmse_final = mean_squared_error(y_test,\n",
    " search_ridge.best_estimator_.predict(X_test), squared = False)\n",
    "print('Test RMSE (Lasso): ', lasso_rmse_final)\n",
    "print('Test RMSE (Ridge): ', ridge_rmse_final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE (OLS):  3.672945713886665\n",
      "Test RMSE (OLS):  4.884457866912155\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "OLS = LinearRegression()\n",
    "pipe_OLS = Pipeline([\n",
    "    (\"std_tf\", scalar),\n",
    "    (\"model\", OLS)\n",
    "])\n",
    "\n",
    "OLS_scores = cross_val_score(\n",
    "    pipe_OLS,\n",
    "    X_other,\n",
    "    y_other,\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = 10\n",
    ")\n",
    "\n",
    "print('Best CV RMSE (OLS): ',  (-OLS_scores).min())\n",
    "\n",
    "OLS_rmse_final = mean_squared_error(\n",
    "    y_test,\n",
    "    pipe_OLS.fit(X_other, y_other).predict(X_test),\n",
    "    squared = False)\n",
    "print('Test RMSE (OLS): ', OLS_rmse_final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, our final results are\n",
    "\n",
    "| Method | CV RMSE | Test RMSE |\n",
    "|:------:|:------:|:------:|:------:|\n",
    "| LS | 3.673 | 4.885|\n",
    "| Ridge | 4.722 | 4.880 |\n",
    "| Lasso | 4.725 | 4.882 |\n",
    "\n",
    "From these results, it is clear that **least squres had the smallest CV RMSE**, and all models had a similar Test RMSE. However, **Ridge (by a small amount) had the lowest Test RMSE** of all 3 models tested."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Propose a model (or set of models) that seem to perform well on\n",
    "this data set, and justify your answer. Make sure that you are\n",
    "evaluating model performance using validation set error, cross-\n",
    "validation, or some other reasonable alternative, as opposed to\n",
    "using training error.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "We present the same table from part (a):\n",
    "\n",
    "| Method | CV RMSE | Test RMSE |\n",
    "|:------:|:------:|:------:|:------:|\n",
    "| LS | 3.673 | 4.885|\n",
    "| Ridge | 4.722 | 4.880 |\n",
    "| Lasso | 4.725 | 4.882 |\n",
    "\n",
    "We propose that the **Ridge model**:\n",
    "\n",
    "$$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) - \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "as the best model for our data. It has (albeit by a small margin) the lowest Test RMSE; hence, it would be the best model (of those tested) for prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "Does your chosen model involve all of the features in the data\n",
    "set? Why or why not?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad',\n",
       "       'tax', 'ptratio', 'lstat'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_ridge.best_estimator_.feature_names_in_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our Ridge model takes all of our features into the mode; however, one drawback of Ridge regression is that we cannot easily evalute feature importance. Hence, we do not know to what extent our features affect our response variable. OLS and Lasso allow us to do this, so if the the goal is inference, then we may not want to use Ridge.\n",
    "\n",
    "So in the broad sense, **yes our model has all of the features**; however, we do not know to what extend they are useful in predicting `medv`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 5.4.2 (10pts)\n",
    "\n",
    "We will now derive the probability that a given observation is part\n",
    "of a bootstrap sample. Suppose that we obtain a bootstrap sample\n",
    "from a set of n observations.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "What is the probability that the first bootstrap observation is\n",
    "not the jth observation from the original sample? Justify your\n",
    "answer.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "The probability the the first bootstrap observation is not the $j$th observation from the original samle is\n",
    "$$1 - \\frac{1}{n}$$\n",
    "where $n$ is the number of observations. The idea here is that the chance of the first observation being the $j$th observation is $\\frac{1}{n}$. So to get the chance that it is *not* the $j$th observation, we take the compliment.\n",
    "\n",
    "### Part (b)\n",
    "\n",
    "What is the probability that the second bootstrap observation\n",
    "is not the jth observation from the original sample?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "The probability the the second bootstrap observation is not the $j$th observation from the original samle is\n",
    "$$1 - \\frac{1}{n}$$\n",
    "This is by independence of the boostrapping process. Since bootstrap performs repeated sampling with replacement, the draws are independent. Hence, the same probability.\n",
    "\n",
    "### Part (c)\n",
    "\n",
    "Argue that the probability that the jth observation is not in the\n",
    "bootstrap sample is $(1-1/n)^n$.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "The proof follows from basic probability theory of independence. First, note that the probaility of not chooosing a sample is $1 - \\frac{1}{n}$ as shown before. And since the draws are not dependent on each other, the probability of not getting the $j$th observation $n$ times is equal to\n",
    "$$\\prod_{i=1}^n (1 - 1/n)_i = (1 - 1/n) \\cdot (1 - 1/n) \\cdots (1 - 1/n) = (1 - 1/n)^n$$\n",
    "\n",
    "### Part (d)\n",
    "\n",
    "When $n=5$, what is the probability that the jth observation is\n",
    "in the bootstrap sample?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Considering the probability of the jth observation not being in is $(1-1/n)^n$, the chance of it being in is just the complement\n",
    "$$ 1 - (1-1/n)^n = 1 - (1-1/5)^5 = 1 - (4/5)^5 \\approx 67.2\\% $$\n",
    "\n",
    "### Part (e)\n",
    "\n",
    "When $n=100$, what is the probability that the jth observation is\n",
    "in the bootstrap sample?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Same idea as part (d)\n",
    "$$ 1 - (1-1/n)^n = 1 - (1-1/100)^5 = 1 - (99/100)^{100} \\approx 63.4\\% $$\n",
    "\n",
    "### Part (f)\n",
    "\n",
    "When $n=10,000$, what is the probability that the jth observation is\n",
    "in the bootstrap sample?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Same idea as part (d)\n",
    "$$ 1 - (1-1/n)^n = 1 - (1-1/10000)^{10000} \\approx 63.2\\% $$\n",
    "\n",
    "### Part (g)\n",
    "\n",
    "Create a plot that displays, for each integer value of $n$ from 1\n",
    "to 100,000, the probability that the jth observation is in the\n",
    "bootstrap sample. Comment on what you observe.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pr(n):\n",
    "    p = (1 - (1 - 1/n)**n)\n",
    "    return p\n",
    "x = np.linspace(1,100000, 100000)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x,pr(x))\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Probability Convergence\")\n",
    "plt.axhline(0.632, c=\"red\", label=\"0.632\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above plot that the probability rapidly converges to about 63.2\\%. The plot takes $n$ to 100,000 but it is clear that this is entirely unnecessary. Convergence happens much sooner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h)\n",
    "\n",
    "We will now investigate numerically the probability that a boot-\n",
    "strap sample of size $n = 100$ contains the jth observation. Here\n",
    "$j = 4$. We repeatedly create bootstrap samples, and each time\n",
    "we record whether or not the fourth observation is contained in\n",
    "the bootstrap sample. (The code below is presented in R)\n",
    "\n",
    "```\n",
    "store <- rep(NA, 10000)\n",
    "for(i in 1:10000){\n",
    "store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0\n",
    "}\n",
    "mean(store)\n",
    "```\n",
    "\n",
    "#### Solution\n",
    "\n",
    "If we run the code in R in terminal (not shown here), then we get the output:\n",
    "```\n",
    "[1] 0.6411\n",
    "```\n",
    "The idea here, is that the code is creawting 10,000 samples of numbers from 1 to 100 with replacement. Then it is seeing how many times the number 4 occurs in each sample. If it occurs more than 0 times, then it is put into the `store` variable. Taking the average of the variable reveals that about 60\\% of the time, the sample will contain 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISL Exercise 5.4.9 (20pts)\n",
    "\n",
    "We will now consider the Boston housing data once again.\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "Based on this data set, provide an estiamte for the population mean of `medv`. Call this estimate $\\hat{\\mu}$.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop Mean Estimate:  22.532806324110698\n"
     ]
    }
   ],
   "source": [
    "print(\"Pop Mean Estimate: \", Boston['medv'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above code that $\\hat{\\mu} = 22.533$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "Provide an estimate of the standard error of $\\hat{\\mu}$. Interpret this result.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error of mu hat:  0.4084569346972866\n"
     ]
    }
   ],
   "source": [
    "def se_mean(x):\n",
    "    medv = np.array(x)\n",
    "    se = np.std(medv) / np.sqrt(len(medv))\n",
    "    return se\n",
    "print(\"Standard Error of mu hat: \", se_mean(Boston['medv']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate that $\\text{SE}(\\hat{\\mu}) = 0.408$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "Now estimate the standard error of $\\hat{\\mu}$ using the bootstrap. How does this compare to your answer from (b)?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap se:  0.4087345541618287\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_se(data, n_replicates=1000):\n",
    "    sample_mean = np.mean(data)\n",
    "    replicates = np.empty(n_replicates)\n",
    "    \n",
    "    for i in range(n_replicates):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        replicates[i] = np.mean(sample)\n",
    "    \n",
    "    return np.std(replicates)\n",
    "\n",
    "se_pred = bootstrap_se(Boston['medv'])\n",
    "print('Bootstrap se: ', se_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the bootstrap methodology can give a very good estimate of the standard error for the mean. The answer from part (c) is very close to that of part (b)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d)\n",
    "\n",
    "Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of `medv`. Compare it to the results obtained using `t.test(Boston$medv)`.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confidence interval is: [ 21.7153 , 23.3503 ]\n"
     ]
    }
   ],
   "source": [
    "mu_hat = np.mean(Boston['medv'])\n",
    "CI_low = mu_hat - (2*se_pred)\n",
    "CI_high = mu_hat + (2*se_pred)\n",
    "\n",
    "print('The confidence interval is: [', round(CI_low, 4), ',', round(CI_high,4), ']')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `t.test(Boston$medv)` in R delivers us the result\n",
    "\n",
    "`95 percent confidence interval:`\n",
    "\n",
    "`21.73 23.34`\n",
    "\n",
    "Which is very similar to the results we got by utilizing the classic confidence interval formula."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e)\n",
    "\n",
    "Based on this dat set, provide an estimate, $\\hat{\\mu}_{med}$, for the median value of `medv` in the population.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median:  21.2\n"
     ]
    }
   ],
   "source": [
    "med_hat = np.median(Boston['medv'])\n",
    "print('Median: ', med_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we find that $\\hat{\\mu}_{med} = 21.2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f)\n",
    "\n",
    "We now would like to estimate the standard error of $\\hat{\\mu}_{med}$. Unfortunately, there is no simple formula for computing the standard\n",
    "error of the median. Instead, estimate the standard error of the\n",
    "median using the bootstrap. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Median se:  0.38371791396806043\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_se_med(data, n_replicates=1000):\n",
    "    sample_median = np.median(data)\n",
    "    replicates = np.empty(n_replicates)\n",
    "    \n",
    "    for i in range(n_replicates):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        replicates[i] = np.median(sample)\n",
    "    \n",
    "    return np.std(replicates)\n",
    "\n",
    "se_med = bootstrap_se_med(Boston['medv'])\n",
    "print('Bootstrap Median se: ', se_med)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error for $\\hat{\\mu}_{med}$ is about 0.38. This value is relatively small in comparison to the median value of 21.2. Hence, there is not too much variation in the median value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (g)\n",
    "\n",
    "Based on this data set, provide an estimate for the tenth percentile of `medv` in Bvoston census tracts. Call this quantity $\\hat{\\mu}_{0.1}$.\n",
    "\n",
    "#### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenth Percentile:  12.75\n"
     ]
    }
   ],
   "source": [
    "percent_10 = np.percentile(Boston['medv'], 10)\n",
    "print('Tenth Percentile: ', percent_10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we find that $\\hat{\\mu}_{0.1} = 12.75$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (h)\n",
    "\n",
    "Use the bootstrap to estimate the standard error of $\\hat{\\mu}_{0.1}$. Comment on your findings.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Standard Error: 0.5036616895297874\n"
     ]
    }
   ],
   "source": [
    "# More concise bootstrap code\n",
    "quantiles = [Boston['medv'].sample(\n",
    "    n=len(Boston),\n",
    "    replace=True).quantile(0.1) for _ in range(1000)]\n",
    "print('Bootstrap Standard Error:', np.std(quantiles))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of $\\hat{\\mu}_{0.1}$ is about 0.5. Once again, this is relatively small in comparison to our $\\hat{\\mu}_{0.1}$ value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question (20pts)\n",
    "\n",
    "Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \\ldots, (x_N,  y_N)$ drawn at random from a population. Let $\\hat \\beta$ be the least squares estimate. Suppose we have some test data $(\\tilde{x}_1, \\tilde{y}_1), \\ldots, (\\tilde{x}_M, \\tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\\text{train}}(\\beta) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta^T x_i)^2$ and $R_{\\text{test}}(\\beta) = \\frac{1}{M} \\sum_{i=1}^M (\\tilde{y}_i - \\beta^T \\tilde{x}_i)^2$. Show that\n",
    "$$\n",
    "\\operatorname{E}[R_{\\text{train}}(\\hat{\\beta})] < \\operatorname{E}[R_{\\text{test}}(\\hat{\\beta})].\n",
    "$$\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prove this inequality, we would like our choice of $M$ to be arbitrary such that we can set $M=N$. With that, we can then utilize the minimizing nature of our $\\hat{\\beta}$ to show that the two expectations are, in fact, equal. So we propose the Lemma\n",
    "\n",
    "*Lemma 1: $\\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right]$ is the same irrespective of our choice of $M$*.\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "First consider the expected testing MSE\n",
    "\\begin{align*}\n",
    "\\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right] &= \\mathbb{E} \\left[ \\frac{1}{M} \\sum_{i=1}^{M} (\\tilde{y}_i - \\hat{\\beta}^T \\tilde{x}_i)^2    \\right] \\\\\n",
    "&= \\frac{1}{M} \\sum_{i=1}^{M} \\mathbb{E} \\left[ (\\tilde{y}_i - \\hat{\\beta}^T \\tilde{x}_i)^2 \\right ] \\\\\n",
    "\\end{align*}\n",
    "by Linearity of Expectations. Then we can expand the square to get\n",
    "$$ \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\mathbb{E}[\\tilde{y}_i^2] + \\mathbb{E} \\left[(\\hat{\\beta}^T \\tilde{x}_i )^2 \\right] - 2 \\mathbb{E} \\left[\\tilde{y}_i \\tilde{x}_i^T \\hat{\\beta} \\right]  \\right)$$\n",
    "We know that all the data points $(\\tilde{x}_i , \\tilde{y}_i)_{i=1}^{M}$ are drawn from the same population. Because of this fact, we can justify dropping the tilde from our variables. Namely, we can replace $\\mathbb{E}[\\tilde{y}_i^2] \\ \\forall i = 1,\\dots,M$ with $\\mathbb{E}[y^2]$. In the same way, we replace $\\mathbb{E}[\\tilde{x}_i^2]$ and $\\mathbb{E}[\\tilde{y}\\tilde{x}^T]$ with the their non-tilde counterparts.\n",
    "\n",
    "Furthermore, by Independence and the Law of Iterated Expectations (recall: $\\mathbb{E}(X) = \\mathbb{E}(\\mathbb{E} (X | Y))$) we break apart the expectation of a product into a product of expecations:\n",
    "$$\\mathbb{E}\\left[(\\hat{\\beta}^T \\tilde{x}_i)^2 \\right] = \\mathbb{E}\\left[\\hat{\\beta}^T \\right] \\mathbb{E}[x^2]$$\n",
    "and\n",
    "$$\\left[\\tilde{y}_i \\tilde{x}_i^T \\hat{\\beta} \\right] = \\mathbb{E}[xy]^T \\mathbb{E}\\left[\\hat{\\beta} \\right]$$\n",
    "We can substitute these expressions back into our previous expression to get\n",
    "$$ \\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right] = \\frac{1}{M} \\mathbb{E}[y^2] + \\mathbb{E}\\left[\\hat{\\beta}^T \\right] \\mathbb{E}[x^2] +  \\mathbb{E}[xy]^T \\mathbb{E}\\left[\\hat{\\beta} \\right] $$\n",
    "Hence, our expected MSE does not depend on our choice of $M$. This concludes the proof of Lemma.\n",
    "\n",
    "Using Lemma 1, without loss of generality, we can take $M=N$ and re-write our testing MSE as\n",
    "$$\\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right] = \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T \\tilde{x}_i)^2    \\right]$$\n",
    "We know, from basic properties of linear regression, that $\\hat{\\beta}$ would not minimize the testing RSS (or SSE); rather, it would be some OLS estimate $\\hat{\\beta}_{test}$ which we would get by conducting OLS on the testing data. From that, we then say that the MSE using the incorrect $\\hat{\\beta}$ estimate must be strictly greater than the correct Testing OLS coefficient:\n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T \\tilde{x}_i)^2 > \\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T_{test} \\tilde{x}_i)^2$$\n",
    "By basic properites of expectations, we know that the same holds true for their respective expected values\n",
    "\\begin{align*}\n",
    "\\mathbb{E} \\left[\\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T \\tilde{x}_i)^2 \\right] &> \\mathbb{E} \\left[\\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T_{test} \\tilde{x}_i)^2 \\right] \\\\\n",
    "\\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right] &> \\mathbb{E} \\left[\\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T_{test} \\tilde{x}_i)^2 \\right]\n",
    "\\end{align*}\n",
    "We now turn our attention to the right side of this inequality. Namely, we recall that $(\\tilde{x}_i , \\tilde{y}_i)_{i=1}^{N}$ is an arbitrary set of points from a larger population, and the OLS testing coefficient associated with it is simply a result of this arbitrary choice of $N$. Hence, we can say that $(\\tilde{x}_i , \\tilde{y}_i)_{i=1}^{N}$ and $(x_i, y_i)_{i=1}^{N}$ have the same distribution and thus the same expectation. This results in\n",
    "\\begin{align*}\n",
    "\\mathbb{E} \\left[\\frac{1}{N} \\sum_{i=1}^{N} (\\tilde{y}_i - \\hat{\\beta}^T_{test} \\tilde{x}_i)^2 \\right] &= \\mathbb{E} \\left[\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{\\beta}^T x_i)^2 \\right] \\\\\n",
    "&= \\mathbb{E}\\left[R_{\\text{train}}(\\hat{\\beta}) \\right]\n",
    "\\end{align*}\n",
    "Putting the two results back together, we prove the statement\n",
    "$$\\mathbb{E}\\left[R_{\\text{train}}(\\hat{\\beta}) \\right] < \\mathbb{E} \\left[R_{test} (\\hat{\\beta}) \\right]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31df7c7627a67be450e0b97ef203135bae9a88aae24921ec8b8b14d7d7918e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
