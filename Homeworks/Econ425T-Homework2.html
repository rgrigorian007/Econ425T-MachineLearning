<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Richard Grigorian (UID: 505-088-797)">
<meta name="dcterms.date" content="2023-01-31">

<title>Econ 425T Homework 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="Econ425T-Homework2_files/libs/clipboard/clipboard.min.js"></script>
<script src="Econ425T-Homework2_files/libs/quarto-html/quarto.js"></script>
<script src="Econ425T-Homework2_files/libs/quarto-html/popper.min.js"></script>
<script src="Econ425T-Homework2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Econ425T-Homework2_files/libs/quarto-html/anchor.min.js"></script>
<link href="Econ425T-Homework2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Econ425T-Homework2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Econ425T-Homework2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Econ425T-Homework2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Econ425T-Homework2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#least-squares-is-mle" id="toc-least-squares-is-mle" class="nav-link active" data-scroll-target="#least-squares-is-mle"><span class="toc-section-number">1</span>  Least squares is MLE</a></li>
  <li><a href="#isl-exercise-6.6.1-10pts" id="toc-isl-exercise-6.6.1-10pts" class="nav-link" data-scroll-target="#isl-exercise-6.6.1-10pts"><span class="toc-section-number">2</span>  ISL Exercise 6.6.1 (10pts)</a>
  <ul class="collapse">
  <li><a href="#part-a" id="toc-part-a" class="nav-link" data-scroll-target="#part-a"><span class="toc-section-number">2.1</span>  Part (a)</a></li>
  <li><a href="#part-b" id="toc-part-b" class="nav-link" data-scroll-target="#part-b"><span class="toc-section-number">2.2</span>  Part (b)</a></li>
  <li><a href="#part-c-true-or-false" id="toc-part-c-true-or-false" class="nav-link" data-scroll-target="#part-c-true-or-false"><span class="toc-section-number">2.3</span>  Part (c) True or False:</a></li>
  </ul></li>
  <li><a href="#isl-exercise-6.6.3-10pts" id="toc-isl-exercise-6.6.3-10pts" class="nav-link" data-scroll-target="#isl-exercise-6.6.3-10pts"><span class="toc-section-number">3</span>  ISL Exercise 6.6.3 (10pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-1" id="toc-part-a-1" class="nav-link" data-scroll-target="#part-a-1"><span class="toc-section-number">3.1</span>  Part (a)</a></li>
  <li><a href="#part-b-1" id="toc-part-b-1" class="nav-link" data-scroll-target="#part-b-1"><span class="toc-section-number">3.2</span>  Part (b)</a></li>
  <li><a href="#part-c" id="toc-part-c" class="nav-link" data-scroll-target="#part-c"><span class="toc-section-number">3.3</span>  Part (c)</a></li>
  <li><a href="#part-d" id="toc-part-d" class="nav-link" data-scroll-target="#part-d"><span class="toc-section-number">3.4</span>  Part (d)</a></li>
  <li><a href="#part-e" id="toc-part-e" class="nav-link" data-scroll-target="#part-e"><span class="toc-section-number">3.5</span>  Part (e)</a></li>
  </ul></li>
  <li><a href="#isl-exercise-6.6.4-10pts" id="toc-isl-exercise-6.6.4-10pts" class="nav-link" data-scroll-target="#isl-exercise-6.6.4-10pts"><span class="toc-section-number">4</span>  ISL Exercise 6.6.4 (10pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-2" id="toc-part-a-2" class="nav-link" data-scroll-target="#part-a-2"><span class="toc-section-number">4.1</span>  Part (a)</a></li>
  <li><a href="#part-b-2" id="toc-part-b-2" class="nav-link" data-scroll-target="#part-b-2"><span class="toc-section-number">4.2</span>  Part (b)</a></li>
  <li><a href="#part-c-1" id="toc-part-c-1" class="nav-link" data-scroll-target="#part-c-1"><span class="toc-section-number">4.3</span>  Part (c)</a></li>
  <li><a href="#part-d-1" id="toc-part-d-1" class="nav-link" data-scroll-target="#part-d-1"><span class="toc-section-number">4.4</span>  Part (d)</a></li>
  <li><a href="#part-e-1" id="toc-part-e-1" class="nav-link" data-scroll-target="#part-e-1"><span class="toc-section-number">4.5</span>  Part (e)</a></li>
  </ul></li>
  <li><a href="#isl-exercise-6.6.5-10pts" id="toc-isl-exercise-6.6.5-10pts" class="nav-link" data-scroll-target="#isl-exercise-6.6.5-10pts"><span class="toc-section-number">5</span>  ISL Exercise 6.6.5 (10pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-3" id="toc-part-a-3" class="nav-link" data-scroll-target="#part-a-3"><span class="toc-section-number">5.1</span>  Part (a)</a></li>
  <li><a href="#part-b-3" id="toc-part-b-3" class="nav-link" data-scroll-target="#part-b-3"><span class="toc-section-number">5.2</span>  Part (b)</a></li>
  <li><a href="#part-c-2" id="toc-part-c-2" class="nav-link" data-scroll-target="#part-c-2"><span class="toc-section-number">5.3</span>  Part (c)</a></li>
  <li><a href="#part-d-2" id="toc-part-d-2" class="nav-link" data-scroll-target="#part-d-2"><span class="toc-section-number">5.4</span>  Part (d)</a></li>
  </ul></li>
  <li><a href="#isl-exercise-6.6.11-30pts" id="toc-isl-exercise-6.6.11-30pts" class="nav-link" data-scroll-target="#isl-exercise-6.6.11-30pts"><span class="toc-section-number">6</span>  ISL Exercise 6.6.11 (30pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-4" id="toc-part-a-4" class="nav-link" data-scroll-target="#part-a-4"><span class="toc-section-number">6.1</span>  Part (a)</a></li>
  <li><a href="#part-b-4" id="toc-part-b-4" class="nav-link" data-scroll-target="#part-b-4"><span class="toc-section-number">6.2</span>  Part (b)</a></li>
  <li><a href="#part-c-3" id="toc-part-c-3" class="nav-link" data-scroll-target="#part-c-3"><span class="toc-section-number">6.3</span>  Part (c)</a></li>
  </ul></li>
  <li><a href="#isl-exercise-5.4.2-10pts" id="toc-isl-exercise-5.4.2-10pts" class="nav-link" data-scroll-target="#isl-exercise-5.4.2-10pts"><span class="toc-section-number">7</span>  ISL Exercise 5.4.2 (10pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-5" id="toc-part-a-5" class="nav-link" data-scroll-target="#part-a-5"><span class="toc-section-number">7.1</span>  Part (a)</a></li>
  <li><a href="#part-b-5" id="toc-part-b-5" class="nav-link" data-scroll-target="#part-b-5"><span class="toc-section-number">7.2</span>  Part (b)</a></li>
  <li><a href="#part-c-4" id="toc-part-c-4" class="nav-link" data-scroll-target="#part-c-4"><span class="toc-section-number">7.3</span>  Part (c)</a></li>
  <li><a href="#part-d-3" id="toc-part-d-3" class="nav-link" data-scroll-target="#part-d-3"><span class="toc-section-number">7.4</span>  Part (d)</a></li>
  <li><a href="#part-e-2" id="toc-part-e-2" class="nav-link" data-scroll-target="#part-e-2"><span class="toc-section-number">7.5</span>  Part (e)</a></li>
  <li><a href="#part-f" id="toc-part-f" class="nav-link" data-scroll-target="#part-f"><span class="toc-section-number">7.6</span>  Part (f)</a></li>
  <li><a href="#part-g" id="toc-part-g" class="nav-link" data-scroll-target="#part-g"><span class="toc-section-number">7.7</span>  Part (g)</a></li>
  <li><a href="#part-h" id="toc-part-h" class="nav-link" data-scroll-target="#part-h"><span class="toc-section-number">7.8</span>  Part (h)</a></li>
  </ul></li>
  <li><a href="#isl-exercise-5.4.9-20pts" id="toc-isl-exercise-5.4.9-20pts" class="nav-link" data-scroll-target="#isl-exercise-5.4.9-20pts"><span class="toc-section-number">8</span>  ISL Exercise 5.4.9 (20pts)</a>
  <ul class="collapse">
  <li><a href="#part-a-6" id="toc-part-a-6" class="nav-link" data-scroll-target="#part-a-6"><span class="toc-section-number">8.1</span>  Part (a)</a></li>
  <li><a href="#part-b-6" id="toc-part-b-6" class="nav-link" data-scroll-target="#part-b-6"><span class="toc-section-number">8.2</span>  Part (b)</a></li>
  <li><a href="#part-c-5" id="toc-part-c-5" class="nav-link" data-scroll-target="#part-c-5"><span class="toc-section-number">8.3</span>  Part (c)</a></li>
  <li><a href="#part-d-4" id="toc-part-d-4" class="nav-link" data-scroll-target="#part-d-4"><span class="toc-section-number">8.4</span>  Part (d)</a></li>
  <li><a href="#part-e-3" id="toc-part-e-3" class="nav-link" data-scroll-target="#part-e-3"><span class="toc-section-number">8.5</span>  Part (e)</a></li>
  <li><a href="#part-f-1" id="toc-part-f-1" class="nav-link" data-scroll-target="#part-f-1"><span class="toc-section-number">8.6</span>  Part (f)</a></li>
  <li><a href="#part-g-1" id="toc-part-g-1" class="nav-link" data-scroll-target="#part-g-1"><span class="toc-section-number">8.7</span>  Part (g)</a></li>
  <li><a href="#part-h-1" id="toc-part-h-1" class="nav-link" data-scroll-target="#part-h-1"><span class="toc-section-number">8.8</span>  Part (h)</a></li>
  </ul></li>
  <li><a href="#bonus-question-20pts" id="toc-bonus-question-20pts" class="nav-link" data-scroll-target="#bonus-question-20pts"><span class="toc-section-number">9</span>  Bonus question (20pts)</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Econ 425T Homework 2</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Richard Grigorian (UID: 505-088-797) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 31, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div style="page-break-after: always;"></div>
<section id="least-squares-is-mle" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="least-squares-is-mle"><span class="header-section-number">1</span> Least squares is MLE</h2>
<p>Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and <span class="math inline">C_p</span> and AIC are equivalent.</p>
<section id="solution" class="level4">
<h4 class="anchored" data-anchor-id="solution">Solution</h4>
<p>To prove the above statement, we separate the proof into two pieces. First, we will show that maximum likelihood and least squares result in the same <span class="math inline">\beta</span> coefficient under linear models with Gaussian errors. Then we will show the equivlanece of <span class="math inline">C_p</span> and AIC.</p>
<p><strong>Part I:</strong></p>
<p>We want to show that <span class="math display"> \hat{\beta}_{ML} = \underset{\beta \in \mathbb{R}^{p+1}}{\operatorname{arg \ max}} \enspace \mathcal{L}(\beta) = (X'X)^{-1}X'Y = \hat{\beta}_{OLS}</span> Under linear models, we assume Linearity such that <span class="math display">\mathbb{E}[Y | X_1 = x_1 , \dots, X_p = x_p] = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p</span> Normal (Gaussian) errors imply that <span class="math inline">\varepsilon \sim N(0,\sigma^2)</span>. Hence, for a sample <span class="math inline">\{(x_i, y_i) \}^n_{i=1}</span>, we say <span class="math inline">Y|X \sim N(X'\beta, \sigma^2 I)</span> where <span class="math inline">I_{n\times n}</span> is the Identity matrix. Since <span class="math inline">Y_1 , \dots, Y_n</span> is i.i.d., we obtain the likelihood function <span class="math display"> \mathcal{L} = \prod^n_{i=1} \phi (Y_i ; (X'\beta)_i, \sigma^2 I) = \prod^n_{i=1} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{\frac{-(Y_i -X'\beta)^2}{2 \sigma^2}} </span> We can re-write this the likelihood function in matrix form as <span class="math display"> \mathcal{L} = \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(\frac{-(Y-X\beta)'(Y-X\beta)}{2 \sigma^2}\right)</span> Where <span class="math inline">Y_{n \times 1}</span> and <span class="math inline">X_{n \times p}</span>. We can then take the logarithm of both sides to get the log-likelihood function: <span class="math display">\begin{align*}
\ln (\mathcal{L}) &amp;= - \frac{n}{2} \ln(2 \pi) - \frac {n}{2} \ln(\sigma^2) - \frac{1}{2 \sigma^2} (Y-X\beta)'(Y-X\beta) \\
&amp;= - \frac{n}{2} \ln(2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2 \sigma^2} (Y'Y - 2\beta X'Y + \beta'X'X\beta)
\end{align*}</span> Since the transpose of a scalar is the scalar <span class="math inline">Y'X\beta = (Y'X\beta)' = \beta'X'Y</span>. We now take the partial derivative with respect to <span class="math inline">\beta</span>: <span class="math display">\begin{align*}
\frac{\partial \ln (\mathcal{L})}{\partial \beta} &amp;= - \frac{1}{2 \sigma^2} \left ( \frac{\partial [Y'Y - 2\beta X'Y + \beta'X'X\beta]}{\partial \beta} \right) = 0 \\
&amp;= - \frac{1}{2 \sigma^2} (-2X'Y + 2X'X\beta) = \frac{1}{\sigma^2} (X'Y - X'X\beta) = 0
\end{align*}</span> This then implies that <span class="math display">\hat{\beta}_{ML} = (X'X)^{-1}X'Y = \hat{\beta}_{OLS}</span></p>
<p><strong>Part II:</strong></p>
<p>We now want to show the equivalence between <span class="math inline">C_p</span> and AIC under the same assumptions as in Part (I). We know from lecture that <span class="math display">C_p = \frac{1}{n} (\text{RSS} + 2d \hat{\sigma}^2)</span> And <span class="math display">\text{AIC} = -  2 \ln \mathcal{L} + 2d</span> where <span class="math inline">d</span> is the total number of parameters used and <span class="math inline">\hat{\sigma}^2</span> is an estimate of the error variance <span class="math inline">\text{Var}(\varepsilon)</span>. Hence, we substitute in our expression for <span class="math inline">\mathcal{L}</span> from Part (I): <span class="math display">\begin{align*}
\text{AIC} &amp;= -  2 \ln \mathcal{L} + 2d \\
&amp;= -2 \left( - \frac{n}{2} \ln(2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{(Y - \hat{Y})'(Y - \hat{Y})}{2 \sigma^2} \right) + 2d \\
&amp;= n \ln (2\pi \sigma^2) + \frac{(Y - \hat{Y})'(Y - \hat{Y})}{\sigma^2} + 2d \\
&amp;= n \ln (2\pi \sigma^2) + \frac{\text{RSS}}{\sigma^2} + 2d \\
&amp;= n \ln (2\pi \sigma^2) + \frac{n}{\sigma^2} C_p
\end{align*}</span> Notice, that AIC only differs by <span class="math inline">C_p</span> by constant scalars <span class="math inline">n</span> and <span class="math inline">\sigma</span>. Therefore, both metrics will coincide in terms of model selection. Hence, we show that the <span class="math inline">C_p</span> and AIC are equivalent in this sense.</p>
</section>
</section>
<section id="isl-exercise-6.6.1-10pts" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="isl-exercise-6.6.1-10pts"><span class="header-section-number">2</span> ISL Exercise 6.6.1 (10pts)</h2>
<p>We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain <span class="math inline">p + 1</span> models, containing <span class="math inline">0,1,2,\dots,p</span> predictors. Explain your answers:</p>
<section id="part-a" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="part-a"><span class="header-section-number">2.1</span> Part (a)</h3>
<p>Which of the three models with <span class="math inline">k</span> predictors has the smallest training RSS?</p>
<section id="solution-1" class="level4">
<h4 class="anchored" data-anchor-id="solution-1">Solution</h4>
<p>Best subset selection will be the model with the smallest training RSS. This is due to the fact that the other 2 approaches conduct model selection dependent upon their starting predictor. In the sense that, if forward/backward selection chooses a different starting variable, the final model may be different. Hence, the RSS will be smallest for Best subset which runs through ever possible combination of model features.</p>
</section>
</section>
<section id="part-b" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="part-b"><span class="header-section-number">2.2</span> Part (b)</h3>
<p>Which of the three models with <span class="math inline">k</span> predictors has the smallest test RSS?</p>
<section id="solution-2" class="level4">
<h4 class="anchored" data-anchor-id="solution-2">Solution</h4>
<p>Best subset selection will likely be the model also with the smallest testing RSS. Once again, it is due to the construction of best subset where it will consider a much greater selection of models. In fact, it considers all combinations of model features. Hence, it is likely that it will have a smaller test RSS than either forward/backward stepwise selection.</p>
</section>
</section>
<section id="part-c-true-or-false" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="part-c-true-or-false"><span class="header-section-number">2.3</span> Part (c) True or False:</h3>
<ol type="1">
<li>The predictors in the <span class="math inline">k</span>-variable model identified by forward stepwise are a subset of the predictors in the <span class="math inline">(k+1)</span> variable model identified by forward stepwise selection.</li>
</ol>
<p>This is <strong>True</strong>. This is literally how forward stepwise selection works. It has a base model (with 1 feature) and then it iteratively adds features and drops those that become statistically insignificant. hence, the <span class="math inline">k</span> feature model will be a subset of the <span class="math inline">k+1</span> feature model.</p>
<ol start="2" type="1">
<li>The predictors in the <span class="math inline">k</span>-variable model identified by backward stepwise are a subset of the predictors in the <span class="math inline">(k + 1)</span> variable model identified by backward stepwise selection.</li>
</ol>
<p>This is <strong>True</strong>. Once again the model with <span class="math inline">k</span> features is just the model with <span class="math inline">k+1</span> features except we have removed a variable. Hence, it is a subset.</p>
<ol start="3" type="1">
<li>The predictors in the <span class="math inline">k</span>-variable model identified by backward stepwise are a subset of the predictors in the <span class="math inline">(k + 1)</span> variable model identified by forward stepwise selection.</li>
</ol>
<p>This is <strong>False</strong>. Forward and Backward selection are not linked in this kind of way. They are also somewhat opposite processes, so the features in the <span class="math inline">k</span> variable model of one method is not guaranteed to be a subset of the <span class="math inline">k+1</span> feature model of the other method.</p>
<ol start="4" type="1">
<li>The predictors in the <span class="math inline">k</span>-variable model identified by forward stepwise are a subset of the predictors in the <span class="math inline">(k+1)</span> variable model identified by backward stepwise selection.</li>
</ol>
<p>This is <strong>False</strong> for the same reason. Forward and Backward selection are not linked in this kind of way. They are also somewhat opposite processes, so the features in the <span class="math inline">k</span> variable model of one method is not guaranteed to be a subset of the <span class="math inline">k+1</span> feature model of the other method.</p>
<ol start="5" type="1">
<li>The predictors in the <span class="math inline">k</span>-variable model identified by best subset are a subset of the predictors in the <span class="math inline">(k + 1)</span> variable model identified by best subset selection.</li>
</ol>
<p>This is <strong>False</strong>. This is not necessarily true because best subset tests every combination of features from the available set of features. By increasing the set of features by 1, it is possible that best subset selection will arrive at a combination of features that were not all simultaneously chosen by the <span class="math inline">k</span> case.</p>
</section>
</section>
<section id="isl-exercise-6.6.3-10pts" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="isl-exercise-6.6.3-10pts"><span class="header-section-number">3</span> ISL Exercise 6.6.3 (10pts)</h2>
<p>Suppose we estimate the regression coefficients in a linear regression model by minimizing</p>
<p><span class="math display">\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr)\text{ subject to }\sum_{j=1}^p|\beta_j|\le s</span></p>
<p>for a particular value of <span class="math inline">s</span>. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.</p>
<section id="part-a-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="part-a-1"><span class="header-section-number">3.1</span> Part (a)</h3>
<p>As we increase <span class="math inline">s</span> from 0. the training RSS will:</p>
<ol type="i">
<li><p>Increase initially, and then eventually start decreasing in an inverted U shape.</p></li>
<li><p>Decrease initially, and then eventually start increasing in a U shape</p></li>
<li><p>Steadily increase</p></li>
<li><p>Steadily decrease</p></li>
<li><p>Remain constant.</p></li>
</ol>
<section id="solution-3" class="level4">
<h4 class="anchored" data-anchor-id="solution-3">Solution</h4>
<p><strong>iv.</strong> is correct. The training RSS will steadily decrease as we increase <span class="math inline">s</span> from 0. As we increas <span class="math inline">s</span> we are weakening our restriction on what our <span class="math inline">\beta</span> coefficients can be; therefore, the model is becoming more flexible which causes a steady decrease in training RSS.</p>
</section>
</section>
<section id="part-b-1" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="part-b-1"><span class="header-section-number">3.2</span> Part (b)</h3>
<p>Repeat (a) for test RSS.</p>
<section id="solution-4" class="level4">
<h4 class="anchored" data-anchor-id="solution-4">Solution</h4>
<p><strong>ii.</strong> is correct. We call on the same idea as before. As we weaken the restriction, the coefficients are allowed to increase to their least squares estimates. This is what causes the testing RSS to initially decrease before increasing once again when the coefficients start overfitting and the variance increases.</p>
</section>
</section>
<section id="part-c" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="part-c"><span class="header-section-number">3.3</span> Part (c)</h3>
<p>Repeat (a) for variance.</p>
<section id="solution-5" class="level4">
<h4 class="anchored" data-anchor-id="solution-5">Solution</h4>
<p><strong>iii.</strong> is correct. As hinted to in the previous part, when <span class="math inline">s=0</span> the coefficients will have no variance. As the restriction is loosened, the model coefficients begin to increase and the coefficients become increasingly fitted to the training data. Hence, the variance will increase.</p>
</section>
</section>
<section id="part-d" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="part-d"><span class="header-section-number">3.4</span> Part (d)</h3>
<p>Repeat (a) for (squared) bias.</p>
<section id="solution-6" class="level4">
<h4 class="anchored" data-anchor-id="solution-6">Solution</h4>
<p><strong>iv.</strong> is correct. We call on the classic bias-variance trade-off. As we loosen the restriction, we are imposing less and less shrinkage; hence, the bias decreases monotonically.</p>
</section>
</section>
<section id="part-e" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="part-e"><span class="header-section-number">3.5</span> Part (e)</h3>
<p>Repeat (a) for the irreducible error.</p>
<section id="solution-7" class="level4">
<h4 class="anchored" data-anchor-id="solution-7">Solution</h4>
<p><strong>v.</strong> is correct. By definition, the irreducible error is due to the underlying data rather than any choice of model or restriction. Hence, it will always stay constant regardless of whatever restrictions we impose.</p>
</section>
</section>
</section>
<section id="isl-exercise-6.6.4-10pts" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="isl-exercise-6.6.4-10pts"><span class="header-section-number">4</span> ISL Exercise 6.6.4 (10pts)</h2>
<p>Suppose we estimate the regression coefficients in a linear regression model by minimizing</p>
<p><span class="math display">\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p\beta_j^2</span></p>
<p>for a particular value of <span class="math inline">\lambda</span>. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.</p>
<section id="part-a-2" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="part-a-2"><span class="header-section-number">4.1</span> Part (a)</h3>
<p>As we increase <span class="math inline">\lambda</span> from 0. the training RSS will:</p>
<ol type="i">
<li><p>Increase initially, and then eventually start decreasing in an inverted U shape.</p></li>
<li><p>Decrease initially, and then eventually start increasing in a U shape</p></li>
<li><p>Steadily increase</p></li>
<li><p>Steadily decrease</p></li>
<li><p>Remain constant.</p></li>
</ol>
<section id="solution-8" class="level4">
<h4 class="anchored" data-anchor-id="solution-8">Solution</h4>
<p><strong>iii.</strong> is correct. The training RSS will steadily increase as we increase <span class="math inline">\lambda</span> from 0. As we increas <span class="math inline">\lambda</span> we are strenghtening our restriction on what our <span class="math inline">\beta</span> coefficients can be; therefore, the model is becoming less flexible which causes a steady increase in training RSS.</p>
</section>
</section>
<section id="part-b-2" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="part-b-2"><span class="header-section-number">4.2</span> Part (b)</h3>
<p>Repeat (a) for test RSS.</p>
<section id="solution-9" class="level4">
<h4 class="anchored" data-anchor-id="solution-9">Solution</h4>
<p><strong>ii.</strong> is correct. We call on the same idea as before. As we strenghten the restriction, the coefficients are allowed to deviate further and further from their Least Squares estimates. Hence, as we increase <span class="math inline">\lambda</span>, the coefficients start being reduced further and further to zero and the overfitting is reduced. Thus, test RSS initially decreases but then when all coefficients are approaching 0, the model is clearly not correct and we expect test RSS to increase.</p>
</section>
</section>
<section id="part-c-1" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="part-c-1"><span class="header-section-number">4.3</span> Part (c)</h3>
<p>Repeat (a) for variance.</p>
<section id="solution-10" class="level4">
<h4 class="anchored" data-anchor-id="solution-10">Solution</h4>
<p><strong>iv.</strong> is correct. As hinted to in the previous part, as <span class="math inline">\lambda \to \infty</span> the coefficients will all be shrunken to 0 and hence have no variance. Thus, as <span class="math inline">\lambda</span> increases, we expect variance to monotonically decrease.</p>
</section>
</section>
<section id="part-d-1" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="part-d-1"><span class="header-section-number">4.4</span> Part (d)</h3>
<p>Repeat (a) for (squared) bias.</p>
<section id="solution-11" class="level4">
<h4 class="anchored" data-anchor-id="solution-11">Solution</h4>
<p><strong>iii.</strong> is correct. We call on the classic bias-variance trade-off. As we strengthen the restriction, we are imposing more and more shrinkage; hence, the bias increases monotonically.</p>
</section>
</section>
<section id="part-e-1" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="part-e-1"><span class="header-section-number">4.5</span> Part (e)</h3>
<p>Repeat (a) for the irreducible error.</p>
<section id="solution-12" class="level4">
<h4 class="anchored" data-anchor-id="solution-12">Solution</h4>
<p><strong>v.</strong> is correct. By definition, the irreducible error is due to the underlying data rather than any choice of model or restriction. Hence, it will always stay constant regardless of whatever restrictions we impose.</p>
</section>
</section>
</section>
<section id="isl-exercise-6.6.5-10pts" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="isl-exercise-6.6.5-10pts"><span class="header-section-number">5</span> ISL Exercise 6.6.5 (10pts)</h2>
<p>It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give different coefficient values to correlated variables. We will now explore this property in a very simple setting.</p>
<p>Suppose that <span class="math inline">n=2</span>, <span class="math inline">p=2</span>, <span class="math inline">x_{11} = x_{12}, x_{21} = x_{12}</span>. Furthermore, suppose that <span class="math inline">y_1 + y_2 = 0</span> and <span class="math inline">x_{11} + x_{21} = 0</span>, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: <span class="math inline">\hat{\beta}_0 = 0</span>.</p>
<section id="part-a-3" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="part-a-3"><span class="header-section-number">5.1</span> Part (a)</h3>
<p>Write out the ridge regression optimization problem in this setting.</p>
<section id="solution-13" class="level4">
<h4 class="anchored" data-anchor-id="solution-13">Solution</h4>
<p>Recall that Ridge Regression seeks to find the coefficients that minimize <span class="math display">\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p\beta_j^2</span> Hence, with the given information, we want to minimize: <span class="math display">(y_1 - \hat{\beta}_1x_1 - \hat{\beta}_2x_1)^2 + (y_2 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2 + \lambda(\hat{\beta}_1^2 + \hat{\beta}_2^2)</span></p>
</section>
</section>
<section id="part-b-3" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="part-b-3"><span class="header-section-number">5.2</span> Part (b)</h3>
<p>Argue that in this setting, the ridge coefficient estimates satisfy <span class="math inline">\hat{\beta}_1 = \hat{\beta}_2</span>.</p>
<section id="solution-14" class="level4">
<h4 class="anchored" data-anchor-id="solution-14">Solution</h4>
<p>We accomplish this by taking the partial derivative with respect to both <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span>. Hence, we have <span class="math display">\frac{\partial(\cdot)}{\partial \hat{\beta}_1} = -2x_1 (y - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_1) - 2x_2 (y_2 - \hat{\beta}_1 x_2 - \hat{\beta}_2 x_2) + 2 \lambda \hat{\beta}_1 = 0</span> Which then implies that <span class="math display">\hat{\beta}_1 = \frac{x_1 (y - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_1) + x_2 (y_2 - \hat{\beta}_1 x_2 - \hat{\beta}_2 x_2)}{\lambda}</span> If we then take the partial with respect to <span class="math inline">\hat{\beta}_2</span> we get <span class="math display">\frac{\partial(\cdot)}{\partial \hat{\beta}_2} = -2x_1 (y - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_1) - 2x_2 (y_2 - \hat{\beta}_1 x_2 - \hat{\beta}_2 x_2) + 2 \lambda \hat{\beta}_2 = 0</span> Which then can be re-written as <span class="math display">\hat{\beta}_2 = \frac{x_1 (y - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_1) + x_2 (y_2 - \hat{\beta}_1 x_2 - \hat{\beta}_2 x_2)}{\lambda}</span> This can only be true if <span class="math inline">\hat{\beta}_1 = \hat{\beta}_2</span>,</p>
</section>
</section>
<section id="part-c-2" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="part-c-2"><span class="header-section-number">5.3</span> Part (c)</h3>
<p>Write out the lasso optimization problem in this setting.</p>
<section id="solution-15" class="level4">
<h4 class="anchored" data-anchor-id="solution-15">Solution</h4>
<p>Recall that Lasso seeks to find the coefficients that minimize <span class="math display">\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p |\beta_j |</span> Hence, with the given information, we want to minimize: <span class="math display">(y_1 - \hat{\beta}_1x_1 - \hat{\beta}_2x_1)^2 + (y_2 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2 + \lambda(|\hat{\beta}_1| + |\hat{\beta}_2|)</span></p>
</section>
</section>
<section id="part-d-2" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="part-d-2"><span class="header-section-number">5.4</span> Part (d)</h3>
<p>Aruge that in this setting, the Lasso coefficients <span class="math inline">\hat{\beta}_1</span> and <span class="math inline">\hat{\beta}_2</span> are not unique – in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.</p>
<section id="solution-16" class="level4">
<h4 class="anchored" data-anchor-id="solution-16">Solution</h4>
<p>Our approach to arguing the lack of uniquness begins with us re-writing our Lasso minimization problem in the form in which it was presented in problem 3 (ISL 6.6.3) <span class="math display">(y_1 - \hat{\beta}_1x_1 - \hat{\beta}_2x_1)^2 + (y_2 - \hat{\beta}_1x_2 - \hat{\beta}_2x_2)^2</span> Subject to the constraint <span class="math display"> |\hat{\beta}_1| + |\hat{\beta}_2|\le s </span> Now, we know (from lecture) that the constraint in Lasso geometrically is a diamond at the origin of <span class="math inline">\mathbb{R}^2</span> euclidean plane (<span class="math inline">\hat{\beta}_1 , \hat{\beta}_2</span>) with a distance from the axis equal to the constraint <span class="math inline">s</span>. Beyond this, we can further simplify our expression since our problem parameters allow us to collapse several terms in order to get <span class="math display">2(y_1 - (\hat{\beta}_1 + x_1\hat{\beta}_2))^2\ge 0</span> Now we minimize this problem by taking the partial <span class="math display">\frac{\partial(\cdot)}{\partial (\hat{\beta}_1 + \hat{\beta}_2)} = -4x_1(y_1 - x_1 (\hat{\beta}_1 + \hat{\beta}_2)) = 0</span> Which then simply rearranges to show <span class="math display">\hat{\beta}_1 + \hat{\beta}_2 = \frac{y_1}{x_1}</span> This expression represents a line which is parallel to the edge of the Lasso diamond. Therefore, the entire edge of the diamond that intersects with the lasso function serves as our solution set. This shows that the solution to this particular Lasso problem is not unique. In particular the lasso optimization problem as the set of solutions <span class="math display">\{(\hat{\beta}_1,\hat{\beta}_2) : \hat{\beta}_1 + \hat{\beta}_2 = s, \ \hat{\beta}_1 , \hat{\beta}_2\ge 0\}</span> And the solutions are symmetric for the case where it is less than zero.</p>
</section>
</section>
</section>
<section id="isl-exercise-6.6.11-30pts" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="isl-exercise-6.6.11-30pts"><span class="header-section-number">6</span> ISL Exercise 6.6.11 (30pts)</h2>
<p>You must follow the <a href="https://ucla-econ-425t.github.io/2023winter/slides/06-modelselection/workflow_lasso.html">typical machine learning paradigm</a> to compare <em>at least</em> 3 methods: least squares, lasso, and ridge. Report final results as</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">CV RMSE</th>
<th style="text-align: center;">Test RMSE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Ridge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Lasso</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">…</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>We will now try to predict the per capita crime rate in the <code>Boston</code> data set.</p>
<section id="part-a-4" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="part-a-4"><span class="header-section-number">6.1</span> Part (a)</h3>
<p>Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.</p>
<section id="solution-17" class="level4">
<h4 class="anchored" data-anchor-id="solution-17">Solution</h4>
<p>As specified above, we will use the <em>typical machine learning paradigm</em> to approach this problem. Namely, we will consider least squares, lasso, and ridge.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading Data</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/Boston.csv"</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> requests.get(url).content</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Boston <span class="op">=</span> pd.read_csv(io.StringIO(s.decode(<span class="st">'utf-8'</span>)), index_col <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Boston.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>crim</th>
      <th>zn</th>
      <th>indus</th>
      <th>chas</th>
      <th>nox</th>
      <th>rm</th>
      <th>age</th>
      <th>dis</th>
      <th>rad</th>
      <th>tax</th>
      <th>ptratio</th>
      <th>lstat</th>
      <th>medv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296</td>
      <td>15.3</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Descriptive Statistics</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Boston.describe().T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>crim</th>
      <td>506.0</td>
      <td>3.613524</td>
      <td>8.601545</td>
      <td>0.00632</td>
      <td>0.082045</td>
      <td>0.25651</td>
      <td>3.677083</td>
      <td>88.9762</td>
    </tr>
    <tr>
      <th>zn</th>
      <td>506.0</td>
      <td>11.363636</td>
      <td>23.322453</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>12.500000</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <th>indus</th>
      <td>506.0</td>
      <td>11.136779</td>
      <td>6.860353</td>
      <td>0.46000</td>
      <td>5.190000</td>
      <td>9.69000</td>
      <td>18.100000</td>
      <td>27.7400</td>
    </tr>
    <tr>
      <th>chas</th>
      <td>506.0</td>
      <td>0.069170</td>
      <td>0.253994</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <th>nox</th>
      <td>506.0</td>
      <td>0.554695</td>
      <td>0.115878</td>
      <td>0.38500</td>
      <td>0.449000</td>
      <td>0.53800</td>
      <td>0.624000</td>
      <td>0.8710</td>
    </tr>
    <tr>
      <th>rm</th>
      <td>506.0</td>
      <td>6.284634</td>
      <td>0.702617</td>
      <td>3.56100</td>
      <td>5.885500</td>
      <td>6.20850</td>
      <td>6.623500</td>
      <td>8.7800</td>
    </tr>
    <tr>
      <th>age</th>
      <td>506.0</td>
      <td>68.574901</td>
      <td>28.148861</td>
      <td>2.90000</td>
      <td>45.025000</td>
      <td>77.50000</td>
      <td>94.075000</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <th>dis</th>
      <td>506.0</td>
      <td>3.795043</td>
      <td>2.105710</td>
      <td>1.12960</td>
      <td>2.100175</td>
      <td>3.20745</td>
      <td>5.188425</td>
      <td>12.1265</td>
    </tr>
    <tr>
      <th>rad</th>
      <td>506.0</td>
      <td>9.549407</td>
      <td>8.707259</td>
      <td>1.00000</td>
      <td>4.000000</td>
      <td>5.00000</td>
      <td>24.000000</td>
      <td>24.0000</td>
    </tr>
    <tr>
      <th>tax</th>
      <td>506.0</td>
      <td>408.237154</td>
      <td>168.537116</td>
      <td>187.00000</td>
      <td>279.000000</td>
      <td>330.00000</td>
      <td>666.000000</td>
      <td>711.0000</td>
    </tr>
    <tr>
      <th>ptratio</th>
      <td>506.0</td>
      <td>18.455534</td>
      <td>2.164946</td>
      <td>12.60000</td>
      <td>17.400000</td>
      <td>19.05000</td>
      <td>20.200000</td>
      <td>22.0000</td>
    </tr>
    <tr>
      <th>lstat</th>
      <td>506.0</td>
      <td>12.653063</td>
      <td>7.141062</td>
      <td>1.73000</td>
      <td>6.950000</td>
      <td>11.36000</td>
      <td>16.955000</td>
      <td>37.9700</td>
    </tr>
    <tr>
      <th>medv</th>
      <td>506.0</td>
      <td>22.532806</td>
      <td>9.197104</td>
      <td>5.00000</td>
      <td>17.025000</td>
      <td>21.20000</td>
      <td>25.000000</td>
      <td>50.0000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Since we have worked with the Boston Housing Dataset before (in HW #1), we will omit grpahical summaries. Below we will split our data into the major test and non-test sets.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initial Split into test and non-test sets</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Boston_other, Boston_test <span class="op">=</span> train_test_split(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    Boston,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    train_size<span class="op">=</span> <span class="fl">0.75</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">425</span>, <span class="co"># seed</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>Boston_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(127, 13)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Boston_other.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(379, 13)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate non-test X and y</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_other <span class="op">=</span> Boston_other.drop(<span class="st">'medv'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>y_other <span class="op">=</span> Boston_other.medv</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Test X and y</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> Boston_test.drop(<span class="st">'medv'</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> Boston_test.medv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now move onto the preprocessing step. We do not have categorical variables in our dataset (the only one being <code>chas</code> but it is already binary), hence we do not need to use a OneHotEncoder transformer.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> make_column_transformer</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardization transformer</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>scalar <span class="op">=</span> StandardScaler()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now build our model. Namely, we will focus on the 3 mentioned earlier (Least Squares, Lasso, and Ridge).</p>
</section>
<section id="lasso-and-ridge" class="level4">
<h4 class="anchored" data-anchor-id="lasso-and-ridge">Lasso and Ridge</h4>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso, Ridge</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> Lasso(max_iter <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(max_iter<span class="op">=</span> <span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>pipe_lasso <span class="op">=</span> Pipeline(steps <span class="op">=</span> [</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"std_tf"</span>, scalar),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"model"</span>, lasso)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>pipe_ridge <span class="op">=</span> Pipeline(steps <span class="op">=</span> [</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"std_tf"</span>, scalar),</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"model"</span>, ridge)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now set up our tuning grid in the range of <span class="math inline">10^{-2}</span> to <span class="math inline">10^3</span>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(start <span class="op">=</span> <span class="op">-</span><span class="dv">3</span>, stop <span class="op">=</span> <span class="dv">2</span>, base <span class="op">=</span> <span class="dv">10</span>, num <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>tuned_parameters <span class="op">=</span> {<span class="st">"model__alpha"</span>: alphas}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up CV</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>n_folds <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>search_lasso <span class="op">=</span> GridSearchCV(</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  pipe_lasso, </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  tuned_parameters, </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  cv <span class="op">=</span> n_folds, </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  scoring <span class="op">=</span> <span class="st">"neg_root_mean_squared_error"</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Refit the best model on the whole data set</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  refit <span class="op">=</span> <span class="va">True</span> </span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>n_folds <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>search_ridge <span class="op">=</span> GridSearchCV(</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  pipe_ridge, </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  tuned_parameters, </span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  cv <span class="op">=</span> n_folds, </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  scoring <span class="op">=</span> <span class="st">"neg_root_mean_squared_error"</span>,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  refit <span class="op">=</span> <span class="va">True</span> </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit CV</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>search_lasso.fit(X_other, y_other)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>search_ridge.fit(X_other, y_other)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>GridSearchCV(cv=10,
             estimator=Pipeline(steps=[('std_tf', StandardScaler()),
                                       ('model', Ridge(max_iter=10000))]),
             param_grid={'model__alpha': array([1.00000000e-03, 1.12332403e-03, 1.26185688e-03, 1.41747416e-03,
       1.59228279e-03, 1.78864953e-03, 2.00923300e-03, 2.25701972e-03,
       2.53536449e-03, 2.84803587e-03, 3.19926714e-03, 3.59381366e-03,
       4.03701726e-03, 4.53487851e-03...
       6.89261210e+00, 7.74263683e+00, 8.69749003e+00, 9.77009957e+00,
       1.09749877e+01, 1.23284674e+01, 1.38488637e+01, 1.55567614e+01,
       1.74752840e+01, 1.96304065e+01, 2.20513074e+01, 2.47707636e+01,
       2.78255940e+01, 3.12571585e+01, 3.51119173e+01, 3.94420606e+01,
       4.43062146e+01, 4.97702356e+01, 5.59081018e+01, 6.28029144e+01,
       7.05480231e+01, 7.92482898e+01, 8.90215085e+01, 1.00000000e+02])},
             scoring='neg_root_mean_squared_error')</code></pre>
</div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CV results</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cv_res <span class="op">=</span> pd.DataFrame({</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"alpha"</span>: alphas,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"rmse_lasso"</span>: <span class="op">-</span>search_lasso.cv_results_[<span class="st">"mean_test_score"</span>],</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"rmse_ridge"</span>: <span class="op">-</span>search_ridge.cv_results_[<span class="st">"mean_test_score"</span>]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>sns.relplot(</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> cv_res,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="st">"alpha"</span>,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> <span class="st">"rmse_lasso"</span>,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>  ).<span class="bu">set</span>(</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    xlabel <span class="op">=</span> <span class="st">"alpha"</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    ylabel <span class="op">=</span> <span class="st">"CV RMSE"</span>,</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="st">"Lasso"</span>,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    xscale <span class="op">=</span> <span class="st">"log"</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>sns.relplot(</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> cv_res,</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="st">"alpha"</span>,</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> <span class="st">"rmse_ridge"</span>,</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>  ).<span class="bu">set</span>(</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    xlabel <span class="op">=</span> <span class="st">"alpha"</span>,</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    ylabel <span class="op">=</span> <span class="st">"CV RMSE"</span>,</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="st">"Ridge"</span>,</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    xscale <span class="op">=</span> <span class="st">"log"</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Econ425T-Homework2_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Econ425T-Homework2_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>From the above plots, we can clearly see how the CV RMSE changes based upon the selected constraint parameter (in this case alpha, but mathematically <span class="math inline">\lambda</span>). For Lasso, the minimum seems to be at low alpha values (although a clear minimum is not present). Whereas for Ridge,we see a clear dip around an alpha of 10.</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Best CV RMSE (Lasso): '</span>,  <span class="op">-</span>search_lasso.best_score_)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Best CV RMSE (Ridge): '</span>,  <span class="op">-</span>search_ridge.best_score_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best CV RMSE (Lasso):  4.725006671650144
Best CV RMSE (Ridge):  4.721794901043198</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Finalize model</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Lasso model: "</span>, search_lasso.best_estimator_)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ridge model: "</span>, search_ridge.best_estimator_)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>lasso_rmse_final <span class="op">=</span> mean_squared_error(y_test,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a> search_lasso.best_estimator_.predict(X_test), squared <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>ridge_rmse_final <span class="op">=</span> mean_squared_error(y_test,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a> search_ridge.best_estimator_.predict(X_test), squared <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test RMSE (Lasso): '</span>, lasso_rmse_final)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test RMSE (Ridge): '</span>, ridge_rmse_final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Lasso model:  Pipeline(steps=[('std_tf', StandardScaler()),
                ('model', Lasso(alpha=0.016297508346206444, max_iter=10000))])
Ridge model:  Pipeline(steps=[('std_tf', StandardScaler()),
                ('model', Ridge(alpha=5.462277217684343, max_iter=10000))])
Test RMSE (Lasso):  4.8822192249764385
Test RMSE (Ridge):  4.880408916408217</code></pre>
</div>
</div>
</section>
<section id="least-squares" class="level4">
<h4 class="anchored" data-anchor-id="least-squares">Least Squares</h4>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Regression</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>OLS <span class="op">=</span> LinearRegression()</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>pipe_OLS <span class="op">=</span> Pipeline([</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"std_tf"</span>, scalar),</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"model"</span>, OLS)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>OLS_scores <span class="op">=</span> cross_val_score(</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    pipe_OLS,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    X_other,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    y_other,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    scoring <span class="op">=</span> <span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Best CV RMSE (OLS): '</span>,  (<span class="op">-</span>OLS_scores).<span class="bu">min</span>())</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>OLS_rmse_final <span class="op">=</span> mean_squared_error(</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    y_test,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    pipe_OLS.fit(X_other, y_other).predict(X_test),</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    squared <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test RMSE (OLS): '</span>, OLS_rmse_final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best CV RMSE (OLS):  3.672945713886665
Test RMSE (OLS):  4.884457866912155</code></pre>
</div>
</div>
<p>Hence, our final results are</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">CV RMSE</th>
<th style="text-align: center;">Test RMSE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LS</td>
<td style="text-align: center;">3.673</td>
<td style="text-align: center;">4.885</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Ridge</td>
<td style="text-align: center;">4.722</td>
<td style="text-align: center;">4.880</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Lasso</td>
<td style="text-align: center;">4.725</td>
<td style="text-align: center;">4.882</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>From these results, it is clear that <strong>least squres had the smallest CV RMSE</strong>, and all models had a similar Test RMSE. However, <strong>Ridge (by a small amount) had the lowest Test RMSE</strong> of all 3 models tested.</p>
</section>
</section>
<section id="part-b-4" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="part-b-4"><span class="header-section-number">6.2</span> Part (b)</h3>
<p>Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross- validation, or some other reasonable alternative, as opposed to using training error.</p>
<section id="solution-18" class="level4">
<h4 class="anchored" data-anchor-id="solution-18">Solution</h4>
<p>We present the same table from part (a):</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">CV RMSE</th>
<th style="text-align: center;">Test RMSE</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LS</td>
<td style="text-align: center;">3.673</td>
<td style="text-align: center;">4.885</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Ridge</td>
<td style="text-align: center;">4.722</td>
<td style="text-align: center;">4.880</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Lasso</td>
<td style="text-align: center;">4.725</td>
<td style="text-align: center;">4.882</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>We propose that the <strong>Ridge model</strong>:</p>
<p><span class="math display">\sum_{i=1}^n\Biggl(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\Biggr) - \lambda\sum_{j=1}^p\beta_j^2</span></p>
<p>as the best model for our data. It has (albeit by a small margin) the lowest Test RMSE; hence, it would be the best model (of those tested) for prediction.</p>
</section>
</section>
<section id="part-c-3" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="part-c-3"><span class="header-section-number">6.3</span> Part (c)</h3>
<p>Does your chosen model involve all of the features in the data set? Why or why not?</p>
<section id="solution-19" class="level4">
<h4 class="anchored" data-anchor-id="solution-19">Solution</h4>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>search_ridge.best_estimator_.feature_names_in_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>array(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad',
       'tax', 'ptratio', 'lstat'], dtype=object)</code></pre>
</div>
</div>
<p>We see that our Ridge model takes all of our features into the mode; however, one drawback of Ridge regression is that we cannot easily evalute feature importance. Hence, we do not know to what extent our features affect our response variable. OLS and Lasso allow us to do this, so if the the goal is inference, then we may not want to use Ridge.</p>
<p>So in the broad sense, <strong>yes our model has all of the features</strong>; however, we do not know to what extend they are useful in predicting <code>medv</code>.</p>
</section>
</section>
</section>
<section id="isl-exercise-5.4.2-10pts" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="isl-exercise-5.4.2-10pts"><span class="header-section-number">7</span> ISL Exercise 5.4.2 (10pts)</h2>
<p>We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.</p>
<section id="part-a-5" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="part-a-5"><span class="header-section-number">7.1</span> Part (a)</h3>
<p>What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.</p>
<section id="solution-20" class="level4">
<h4 class="anchored" data-anchor-id="solution-20">Solution</h4>
<p>The probability the the first bootstrap observation is not the <span class="math inline">j</span>th observation from the original samle is <span class="math display">1 - \frac{1}{n}</span> where <span class="math inline">n</span> is the number of observations. The idea here is that the chance of the first observation being the <span class="math inline">j</span>th observation is <span class="math inline">\frac{1}{n}</span>. So to get the chance that it is <em>not</em> the <span class="math inline">j</span>th observation, we take the compliment.</p>
</section>
</section>
<section id="part-b-5" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="part-b-5"><span class="header-section-number">7.2</span> Part (b)</h3>
<p>What is the probability that the second bootstrap observation is not the jth observation from the original sample?</p>
<section id="solution-21" class="level4">
<h4 class="anchored" data-anchor-id="solution-21">Solution</h4>
<p>The probability the the second bootstrap observation is not the <span class="math inline">j</span>th observation from the original samle is <span class="math display">1 - \frac{1}{n}</span> This is by independence of the boostrapping process. Since bootstrap performs repeated sampling with replacement, the draws are independent. Hence, the same probability.</p>
</section>
</section>
<section id="part-c-4" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="part-c-4"><span class="header-section-number">7.3</span> Part (c)</h3>
<p>Argue that the probability that the jth observation is not in the bootstrap sample is <span class="math inline">(1-1/n)^n</span>.</p>
<section id="solution-22" class="level4">
<h4 class="anchored" data-anchor-id="solution-22">Solution</h4>
<p>The proof follows from basic probability theory of independence. First, note that the probaility of not chooosing a sample is <span class="math inline">1 - \frac{1}{n}</span> as shown before. And since the draws are not dependent on each other, the probability of not getting the <span class="math inline">j</span>th observation <span class="math inline">n</span> times is equal to <span class="math display">\prod_{i=1}^n (1 - 1/n)_i = (1 - 1/n) \cdot (1 - 1/n) \cdots (1 - 1/n) = (1 - 1/n)^n</span></p>
</section>
</section>
<section id="part-d-3" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="part-d-3"><span class="header-section-number">7.4</span> Part (d)</h3>
<p>When <span class="math inline">n=5</span>, what is the probability that the jth observation is in the bootstrap sample?</p>
<section id="solution-23" class="level4">
<h4 class="anchored" data-anchor-id="solution-23">Solution</h4>
<p>Considering the probability of the jth observation not being in is <span class="math inline">(1-1/n)^n</span>, the chance of it being in is just the complement <span class="math display"> 1 - (1-1/n)^n = 1 - (1-1/5)^5 = 1 - (4/5)^5 \approx 67.2\% </span></p>
</section>
</section>
<section id="part-e-2" class="level3" data-number="7.5">
<h3 data-number="7.5" class="anchored" data-anchor-id="part-e-2"><span class="header-section-number">7.5</span> Part (e)</h3>
<p>When <span class="math inline">n=100</span>, what is the probability that the jth observation is in the bootstrap sample?</p>
<section id="solution-24" class="level4">
<h4 class="anchored" data-anchor-id="solution-24">Solution</h4>
<p>Same idea as part (d) <span class="math display"> 1 - (1-1/n)^n = 1 - (1-1/100)^5 = 1 - (99/100)^{100} \approx 63.4\% </span></p>
</section>
</section>
<section id="part-f" class="level3" data-number="7.6">
<h3 data-number="7.6" class="anchored" data-anchor-id="part-f"><span class="header-section-number">7.6</span> Part (f)</h3>
<p>When <span class="math inline">n=10,000</span>, what is the probability that the jth observation is in the bootstrap sample?</p>
<section id="solution-25" class="level4">
<h4 class="anchored" data-anchor-id="solution-25">Solution</h4>
<p>Same idea as part (d) <span class="math display"> 1 - (1-1/n)^n = 1 - (1-1/10000)^{10000} \approx 63.2\% </span></p>
</section>
</section>
<section id="part-g" class="level3" data-number="7.7">
<h3 data-number="7.7" class="anchored" data-anchor-id="part-g"><span class="header-section-number">7.7</span> Part (g)</h3>
<p>Create a plot that displays, for each integer value of <span class="math inline">n</span> from 1 to 100,000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.</p>
<section id="solution-26" class="level4">
<h4 class="anchored" data-anchor-id="solution-26">Solution</h4>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pr(n):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">1</span><span class="op">/</span>n)<span class="op">**</span>n)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">1</span>,<span class="dv">100000</span>, <span class="dv">100000</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(x,pr(x))</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"n"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Probability Convergence"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="fl">0.632</span>, c<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"0.632"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Econ425T-Homework2_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It is clear from the above plot that the probability rapidly converges to about 63.2%. The plot takes <span class="math inline">n</span> to 100,000 but it is clear that this is entirely unnecessary. Convergence happens much sooner.</p>
</section>
</section>
<section id="part-h" class="level3" data-number="7.8">
<h3 data-number="7.8" class="anchored" data-anchor-id="part-h"><span class="header-section-number">7.8</span> Part (h)</h3>
<p>We will now investigate numerically the probability that a boot- strap sample of size <span class="math inline">n = 100</span> contains the jth observation. Here <span class="math inline">j = 4</span>. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. (The code below is presented in R)</p>
<pre><code>store &lt;- rep(NA, 10000)
for(i in 1:10000){
store[i] &lt;- sum(sample(1:100, rep=TRUE) == 4) &gt; 0
}
mean(store)</code></pre>
<section id="solution-27" class="level4">
<h4 class="anchored" data-anchor-id="solution-27">Solution</h4>
<p>If we run the code in R in terminal (not shown here), then we get the output:</p>
<pre><code>[1] 0.6411</code></pre>
<p>The idea here, is that the code is creawting 10,000 samples of numbers from 1 to 100 with replacement. Then it is seeing how many times the number 4 occurs in each sample. If it occurs more than 0 times, then it is put into the <code>store</code> variable. Taking the average of the variable reveals that about 60% of the time, the sample will contain 4.</p>
</section>
</section>
</section>
<section id="isl-exercise-5.4.9-20pts" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="isl-exercise-5.4.9-20pts"><span class="header-section-number">8</span> ISL Exercise 5.4.9 (20pts)</h2>
<p>We will now consider the Boston housing data once again.</p>
<section id="part-a-6" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="part-a-6"><span class="header-section-number">8.1</span> Part (a)</h3>
<p>Based on this data set, provide an estiamte for the population mean of <code>medv</code>. Call this estimate <span class="math inline">\hat{\mu}</span>.</p>
<section id="solution-28" class="level4">
<h4 class="anchored" data-anchor-id="solution-28">Solution</h4>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Pop Mean Estimate: "</span>, Boston[<span class="st">'medv'</span>].mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pop Mean Estimate:  22.532806324110698</code></pre>
</div>
</div>
<p>It is clear from the above code that <span class="math inline">\hat{\mu} = 22.533</span></p>
</section>
</section>
<section id="part-b-6" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="part-b-6"><span class="header-section-number">8.2</span> Part (b)</h3>
<p>Provide an estimate of the standard error of <span class="math inline">\hat{\mu}</span>. Interpret this result.</p>
<section id="solution-29" class="level4">
<h4 class="anchored" data-anchor-id="solution-29">Solution</h4>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> se_mean(x):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    medv <span class="op">=</span> np.array(x)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    se <span class="op">=</span> np.std(medv) <span class="op">/</span> np.sqrt(<span class="bu">len</span>(medv))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> se</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Standard Error of mu hat: "</span>, se_mean(Boston[<span class="st">'medv'</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Standard Error of mu hat:  0.4084569346972866</code></pre>
</div>
</div>
<p>We estimate that <span class="math inline">\text{SE}(\hat{\mu}) = 0.408</span>.</p>
</section>
</section>
<section id="part-c-5" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="part-c-5"><span class="header-section-number">8.3</span> Part (c)</h3>
<p>Now estimate the standard error of <span class="math inline">\hat{\mu}</span> using the bootstrap. How does this compare to your answer from (b)?</p>
<section id="solution-30" class="level4">
<h4 class="anchored" data-anchor-id="solution-30">Solution</h4>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap_se(data, n_replicates<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    sample_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    replicates <span class="op">=</span> np.empty(n_replicates)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_replicates):</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> np.random.choice(data, size<span class="op">=</span><span class="bu">len</span>(data), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        replicates[i] <span class="op">=</span> np.mean(sample)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.std(replicates)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>se_pred <span class="op">=</span> bootstrap_se(Boston[<span class="st">'medv'</span>])</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Bootstrap se: '</span>, se_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap se:  0.4087345541618287</code></pre>
</div>
</div>
<p>As seen above, the bootstrap methodology can give a very good estimate of the standard error for the mean. The answer from part (c) is very close to that of part (b).</p>
</section>
</section>
<section id="part-d-4" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="part-d-4"><span class="header-section-number">8.4</span> Part (d)</h3>
<p>Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of <code>medv</code>. Compare it to the results obtained using <code>t.test(Boston$medv)</code>.</p>
<section id="solution-31" class="level4">
<h4 class="anchored" data-anchor-id="solution-31">Solution</h4>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="op">=</span> np.mean(Boston[<span class="st">'medv'</span>])</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>CI_low <span class="op">=</span> mu_hat <span class="op">-</span> (<span class="dv">2</span><span class="op">*</span>se_pred)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>CI_high <span class="op">=</span> mu_hat <span class="op">+</span> (<span class="dv">2</span><span class="op">*</span>se_pred)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The confidence interval is: ['</span>, <span class="bu">round</span>(CI_low, <span class="dv">4</span>), <span class="st">','</span>, <span class="bu">round</span>(CI_high,<span class="dv">4</span>), <span class="st">']'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The confidence interval is: [ 21.7153 , 23.3503 ]</code></pre>
</div>
</div>
<p>Running <code>t.test(Boston$medv)</code> in R delivers us the result</p>
<p><code>95 percent confidence interval:</code></p>
<p><code>21.73 23.34</code></p>
<p>Which is very similar to the results we got by utilizing the classic confidence interval formula.</p>
</section>
</section>
<section id="part-e-3" class="level3" data-number="8.5">
<h3 data-number="8.5" class="anchored" data-anchor-id="part-e-3"><span class="header-section-number">8.5</span> Part (e)</h3>
<p>Based on this dat set, provide an estimate, <span class="math inline">\hat{\mu}_{med}</span>, for the median value of <code>medv</code> in the population.</p>
<section id="solution-32" class="level4">
<h4 class="anchored" data-anchor-id="solution-32">Solution</h4>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>med_hat <span class="op">=</span> np.median(Boston[<span class="st">'medv'</span>])</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Median: '</span>, med_hat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Median:  21.2</code></pre>
</div>
</div>
<p>Thus we find that <span class="math inline">\hat{\mu}_{med} = 21.2</span>.</p>
</section>
</section>
<section id="part-f-1" class="level3" data-number="8.6">
<h3 data-number="8.6" class="anchored" data-anchor-id="part-f-1"><span class="header-section-number">8.6</span> Part (f)</h3>
<p>We now would like to estimate the standard error of <span class="math inline">\hat{\mu}_{med}</span>. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap_se_med(data, n_replicates<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    sample_median <span class="op">=</span> np.median(data)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    replicates <span class="op">=</span> np.empty(n_replicates)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_replicates):</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> np.random.choice(data, size<span class="op">=</span><span class="bu">len</span>(data), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>        replicates[i] <span class="op">=</span> np.median(sample)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.std(replicates)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>se_med <span class="op">=</span> bootstrap_se_med(Boston[<span class="st">'medv'</span>])</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Bootstrap Median se: '</span>, se_med)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap Median se:  0.38371791396806043</code></pre>
</div>
</div>
<p>The standard error for <span class="math inline">\hat{\mu}_{med}</span> is about 0.38. This value is relatively small in comparison to the median value of 21.2. Hence, there is not too much variation in the median value.</p>
</section>
<section id="part-g-1" class="level3" data-number="8.7">
<h3 data-number="8.7" class="anchored" data-anchor-id="part-g-1"><span class="header-section-number">8.7</span> Part (g)</h3>
<p>Based on this data set, provide an estimate for the tenth percentile of <code>medv</code> in Bvoston census tracts. Call this quantity <span class="math inline">\hat{\mu}_{0.1}</span>.</p>
<section id="solution-33" class="level4">
<h4 class="anchored" data-anchor-id="solution-33">Solution</h4>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>percent_10 <span class="op">=</span> np.percentile(Boston[<span class="st">'medv'</span>], <span class="dv">10</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tenth Percentile: '</span>, percent_10)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tenth Percentile:  12.75</code></pre>
</div>
</div>
<p>Hence, we find that <span class="math inline">\hat{\mu}_{0.1} = 12.75</span>.</p>
</section>
</section>
<section id="part-h-1" class="level3" data-number="8.8">
<h3 data-number="8.8" class="anchored" data-anchor-id="part-h-1"><span class="header-section-number">8.8</span> Part (h)</h3>
<p>Use the bootstrap to estimate the standard error of <span class="math inline">\hat{\mu}_{0.1}</span>. Comment on your findings.</p>
<section id="solution-34" class="level4">
<h4 class="anchored" data-anchor-id="solution-34">Solution</h4>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># More concise bootstrap code</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>quantiles <span class="op">=</span> [Boston[<span class="st">'medv'</span>].sample(</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="bu">len</span>(Boston),</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    replace<span class="op">=</span><span class="va">True</span>).quantile(<span class="fl">0.1</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Bootstrap Standard Error:'</span>, np.std(quantiles))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap Standard Error: 0.5036616895297874</code></pre>
</div>
</div>
<p>The standard error of <span class="math inline">\hat{\mu}_{0.1}</span> is about 0.5. Once again, this is relatively small in comparison to our <span class="math inline">\hat{\mu}_{0.1}</span> value.</p>
</section>
</section>
</section>
<section id="bonus-question-20pts" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="bonus-question-20pts"><span class="header-section-number">9</span> Bonus question (20pts)</h2>
<p>Consider a linear regression, fit by least squares to a set of training data <span class="math inline">(x_1, y_1), \ldots, (x_N, y_N)</span> drawn at random from a population. Let <span class="math inline">\hat \beta</span> be the least squares estimate. Suppose we have some test data <span class="math inline">(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)</span> drawn at random from the same population as the training data. If <span class="math inline">R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2</span> and <span class="math inline">R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2</span>. Show that <span class="math display">
\operatorname{E}[R_{\text{train}}(\hat{\beta})] &lt; \operatorname{E}[R_{\text{test}}(\hat{\beta})].
</span></p>
<section id="solution-35" class="level4">
<h4 class="anchored" data-anchor-id="solution-35">Solution</h4>
<p>In order to prove this inequality, we would like our choice of <span class="math inline">M</span> to be arbitrary such that we can set <span class="math inline">M=N</span>. With that, we can then utilize the minimizing nature of our <span class="math inline">\hat{\beta}</span> to show that the two expectations are, in fact, equal. So we propose the Lemma</p>
<p><em>Lemma 1: <span class="math inline">\mathbb{E} \left[R_{test} (\hat{\beta}) \right]</span> is the same irrespective of our choice of <span class="math inline">M</span></em>.</p>
<p><em>Proof</em>:</p>
<p>First consider the expected testing MSE <span class="math display">\begin{align*}
\mathbb{E} \left[R_{test} (\hat{\beta}) \right] &amp;= \mathbb{E} \left[ \frac{1}{M} \sum_{i=1}^{M} (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2    \right] \\
&amp;= \frac{1}{M} \sum_{i=1}^{M} \mathbb{E} \left[ (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2 \right ] \\
\end{align*}</span> by Linearity of Expectations. Then we can expand the square to get <span class="math display"> \frac{1}{M} \sum_{i=1}^{M} \left( \mathbb{E}[\tilde{y}_i^2] + \mathbb{E} \left[(\hat{\beta}^T \tilde{x}_i )^2 \right] - 2 \mathbb{E} \left[\tilde{y}_i \tilde{x}_i^T \hat{\beta} \right]  \right)</span> We know that all the data points <span class="math inline">(\tilde{x}_i , \tilde{y}_i)_{i=1}^{M}</span> are drawn from the same population. Because of this fact, we can justify dropping the tilde from our variables. Namely, we can replace <span class="math inline">\mathbb{E}[\tilde{y}_i^2] \ \forall i = 1,\dots,M</span> with <span class="math inline">\mathbb{E}[y^2]</span>. In the same way, we replace <span class="math inline">\mathbb{E}[\tilde{x}_i^2]</span> and <span class="math inline">\mathbb{E}[\tilde{y}\tilde{x}^T]</span> with the their non-tilde counterparts.</p>
<p>Furthermore, by Independence and the Law of Iterated Expectations (recall: <span class="math inline">\mathbb{E}(X) = \mathbb{E}(\mathbb{E} (X | Y))</span>) we break apart the expectation of a product into a product of expecations: <span class="math display">\mathbb{E}\left[(\hat{\beta}^T \tilde{x}_i)^2 \right] = \mathbb{E}\left[\hat{\beta}^T \right] \mathbb{E}[x^2]</span> and <span class="math display">\left[\tilde{y}_i \tilde{x}_i^T \hat{\beta} \right] = \mathbb{E}[xy]^T \mathbb{E}\left[\hat{\beta} \right]</span> We can substitute these expressions back into our previous expression to get <span class="math display"> \mathbb{E} \left[R_{test} (\hat{\beta}) \right] = \frac{1}{M} \mathbb{E}[y^2] + \mathbb{E}\left[\hat{\beta}^T \right] \mathbb{E}[x^2] +  \mathbb{E}[xy]^T \mathbb{E}\left[\hat{\beta} \right] </span> Hence, our expected MSE does not depend on our choice of <span class="math inline">M</span>. This concludes the proof of Lemma.</p>
<p>Using Lemma 1, without loss of generality, we can take <span class="math inline">M=N</span> and re-write our testing MSE as <span class="math display">\mathbb{E} \left[R_{test} (\hat{\beta}) \right] = \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2    \right]</span> We know, from basic properties of linear regression, that <span class="math inline">\hat{\beta}</span> would not minimize the testing RSS (or SSE); rather, it would be some OLS estimate <span class="math inline">\hat{\beta}_{test}</span> which we would get by conducting OLS on the testing data. From that, we then say that the MSE using the incorrect <span class="math inline">\hat{\beta}</span> estimate must be strictly greater than the correct Testing OLS coefficient: <span class="math display">\frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2 &gt; \frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T_{test} \tilde{x}_i)^2</span> By basic properites of expectations, we know that the same holds true for their respective expected values <span class="math display">\begin{align*}
\mathbb{E} \left[\frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T \tilde{x}_i)^2 \right] &amp;&gt; \mathbb{E} \left[\frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T_{test} \tilde{x}_i)^2 \right] \\
\mathbb{E} \left[R_{test} (\hat{\beta}) \right] &amp;&gt; \mathbb{E} \left[\frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T_{test} \tilde{x}_i)^2 \right]
\end{align*}</span> We now turn our attention to the right side of this inequality. Namely, we recall that <span class="math inline">(\tilde{x}_i , \tilde{y}_i)_{i=1}^{N}</span> is an arbitrary set of points from a larger population, and the OLS testing coefficient associated with it is simply a result of this arbitrary choice of <span class="math inline">N</span>. Hence, we can say that <span class="math inline">(\tilde{x}_i , \tilde{y}_i)_{i=1}^{N}</span> and <span class="math inline">(x_i, y_i)_{i=1}^{N}</span> have the same distribution and thus the same expectation. This results in <span class="math display">\begin{align*}
\mathbb{E} \left[\frac{1}{N} \sum_{i=1}^{N} (\tilde{y}_i - \hat{\beta}^T_{test} \tilde{x}_i)^2 \right] &amp;= \mathbb{E} \left[\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{\beta}^T x_i)^2 \right] \\
&amp;= \mathbb{E}\left[R_{\text{train}}(\hat{\beta}) \right]
\end{align*}</span> Putting the two results back together, we prove the statement <span class="math display">\mathbb{E}\left[R_{\text{train}}(\hat{\beta}) \right] &lt; \mathbb{E} \left[R_{test} (\hat{\beta}) \right]</span></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>